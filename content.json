{"meta":{"title":"缺省的梦境","subtitle":"","description":"","author":"wizardTOT","url":"https://wizardtot.github.io","root":"/"},"pages":[{"title":"about","date":"2024-06-28T07:38:40.000Z","updated":"2025-01-18T04:48:34.786Z","comments":true,"path":"about/index.html","permalink":"https://wizardtot.github.io/about/index.html","excerpt":"","text":"普普通通的计科学生，欢迎恰Q161221879","author":"WizardTOT"},{"title":"所有分类","date":"2025-01-18T04:59:42.163Z","updated":"2025-01-18T04:59:42.163Z","comments":true,"path":"categories/index.html","permalink":"https://wizardtot.github.io/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2025-01-18T05:02:32.474Z","updated":"2025-01-18T05:02:32.474Z","comments":true,"path":"tags/index.html","permalink":"https://wizardtot.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"ceph论文阅读","date":"2025-04-28T15:13:44.905Z","updated":"2025-04-30T07:50:28.491Z","comments":true,"path":"2025/04/28/ceph论文阅读/","permalink":"https://wizardtot.github.io/2025/04/28/ceph%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/","excerpt":"","text":"ceph论文阅读论文原文 https://www3.nd.edu/~dthain/courses/cse40771/spring2007/papers/ceph.pdf ceph，一种为OSD设计的分布式文件系统，目标： 使元数据与数据管理分离，使元数据工作负载分布式处理 提升系统的可扩展性 Overviewceph包含三部分：客户端接口，OSD集群和元数据集群。总体上具有以下特点： 数据与元数据解耦 客户端与元数据集群交互后无需文件列表，直接通过计算存储文件的OSD位置，然后与特定的OSD设备交互。 动态分布式元数据管理 设计动态子树分区动态调整元数据集群的负载，能应对极端的工作负载 自主分布式对象存储 OSD设备间能自主完成数据复制、故障处理等功能，提供客户和元数据集群单一逻辑对象存储。 客户端客户端访问ceph的客户端接口可以直接链接存储系统或FUSE访问，每个客户端维护缓存。客户端打开文件时会向MDS发送请求，然后获得索引节点号、条带化信息、映射信息，以及文件权限。对象名称就是索引节点+条带号组成。无需查询文件列表，客户端即可通过CRUSH算法计算对象的主OSD和副本。然后客户端直接向OSD读写数据。 条带化使客户端并行读写提升性能。ceph的条带化我个人理解就是数据分片，不是存入不同的磁盘而是存入不同的OSD中。对象会在不同的OSD中复制(实际是以多个对象组成的PG)，条带与对象之间独立，对象复制后条带也会复制。客户端读写可以授予缓冲读和缓冲写。关闭后收回权限并且MDS更新元数据。 客户端同步当多个客户端并行访问同一文件MDS将撤销缓冲读写的权限，强制客户端实现POSIX一致性：读操作可以反映之前写操作，写操作是原子性的。有读写的应用都会被阻塞直到被OSD确认，OSD实现更新序列化和一致性。 为了提升高性能计算的性能，ceph可以为一些自行管理一致性的程序放宽一致性，在读写时可选择缓冲或缓存，而不用同步操作；提供了 lazyio_propagate 和 lazyio_synchronize 两个函数，方便应用在需要时手动同步数据，以此在保证基本读写语义的同时，满足分布式应用对性能的需求。 namespace涉及文件系统命名空间的交互，MDS管理使其序列化和一致，无需锁或者租约机制避免回调带来的性能损失。 为提升性能，ceph对于先readdir再对每个文件stat这种常见的访问方式放宽一致性，readir后执行stat会返回短暂缓存的信息。 对于多个客户端写入时的stat操作，MDS暂停客户端更新然后统计最新的文件元数据，回复后再返回写权限，确保一定可串行化。 元数据元数据存储元数据在MDS的内存中缓存，以日志形式更新到ODS的存储中，且日志是幂等更新的。当一个节点故障时，另一个节点能扫描日志恢复故障节点的缓存恢复文件系统状态。 ceph的元数据工作负载具有目录局部性，ceph的文件inode嵌入到目录文件中，索引节点按范围分配给OSD，单次的OSD设备文件读取能预取整个目录。对于硬链接的索引节点，ceph使用辅助锚点表进行寻址。 动态子树分区ceph中元数据缓存是主副本策略管理，对于管理元数据缓存的分配(就是分配MDS节点管理哪部分的元数据)，静态子树分区无法适应元数据的动态工作负载，哈希方法降低了元数据的局部性。 ceph使用动态子树分区分配，在文件的目录树，MDS使用计数器来衡量目录层次中元数据的负载。任何操作都会使受影响的索引节点及其一直到根目录的所有父节点的计数器递增。MDS的工作负载会定期进行比较，迁移部分文件子树保持工作负载的动态分配。 迁移或复制时文件索引会由一致性的要求分为三部分，文件安全信息，文件信息和不可变信息，文件安全信息和文件信息需要锁，有不同的转换规则。 工作负载对于频繁访问区域的工作负载，ceph将区域复制给多个MDS平衡负载，对于热点或突发的工作负载，ceph会将目录下的文件按文件名进行哈希分配，牺牲局部性实现负载的均衡。 对于客户端的负载均衡，客户端可以从MDS中获得其需要的元数据分区情况，客户端能访问访问热度最低的MDS。对于热点的元数据，客户端能得知元数据分配到不同MDS中，将客户端的负载分散。 RADOSCRUSH对象分布和定位ceph在底层设计了可靠自主分布式对象存储(RADOS)，文件对象的复制、集群扩展、故障检测恢复等功能由OSD完成。对象先使用哈希函数分配到不同的PG中，通过位掩码控制PG中对象的数量。再用CRUSH哈希算法将PG分配给OSD。定位一个对象无需建立对象的列表，按照以上流程通过PG映射和OSD集群映射图即可计算一个对象主副本OSD位置。 这张图描述了文件条带化对象、对象到OSD存储的过程。文件条带化为对象后，根据索引节点号ino+对象编号生成对象全局唯一的编号oid，然后对象掩码+哈希分配到PG中，每个PG通过crush算法分到多个OSD存储中复制。 集群映射图就是OSD的逻辑或物理架构。CRUSH算法使PG分配到不在一个机房的OSD中复制。集群映射图还包含非正常工作OSD列表，更新用纪元号标识，OSD之间会共享增量式的映射图更新。 对象复制前面提到ceph按照PG进行主副本复制。复制在OSD之间完成，第一个OSD作为主设备接收到PG后分配一个version，复制的过程类似于2pc，在每个副本OSD应用了更新并向主OSD做出响应之后，主OSD会在本地应用该更新，然后向客户端确认写入操作已完成。客户端在收到主OSD的确认前会缓冲写入操作。 故障检测RADOS中OSD间会自主检查故障，磁盘错误等故障单个OSD能自主检测，OSD在网络上无法访问的故障需要监测。每个OSD监控与其共享的PG复制的OSD状态，可以通过上述的复制消息或者ping监控。如果一个OSD出现以下情况可以认为故障： 该设备在网络上是否可访问 该设备是否被CRUSH算法分配PG 故障的OSD如果为主设备，则其权限交给下一个副本OSD，设备没有恢复，则在映射图中标记并加入新的设备复制内容。客户端未完成的操作redo给新的主设备即可。 数据恢复和集群更新OSD集群映射图会由于故障或新节点加入发生变更，实现快速恢复，每个OSD为每个对象维护版本号，为每个PG维护近期变更日志。当活动OSD接收到更新的集群映射时，会遍历本地存储的所有归置组，通过计算 CRUSH 映射来确定自己作为主设备或从设备所负责的归置组。新运行的OSD需要与PG中其余的OSD进行对等通信。 在一个PG中，OSD会向主OSD提供OSD版本号，而主OSD汇集所有的PG版本号后，若主OSD缺少最新PG状态，会从PG内当前或之前的OSD获取PG近期变更日志，更新最新的PG内容。然后主OSD向副OSD发送更新。 OSD本地文件系统这里文中解释OSD本地为何不适用ext3而是EBOFS。ext3文件系统无法判断一个对象的更新何时commit，ext3是一个日志系统，同步写入和日志记录能提供安全性但带来极大的性能损失。每个OSD在用户空间中使用EBOFS存储，与原始块设备直接交互。基于以下原因： 能自主定义所需的底层存储操作，将更新和提交分离 支持原子事务，并且在内存缓存更新就会返回，并异步通知 在用户空间更灵活，无需与Linux的虚拟文件系统和页缓存交互 更积极写入磁盘，后续的更新也会取消过时未完成的IO操作，调度效率高 基于B树，数据紧凑，能快速找到空闲空间","categories":[],"tags":[]},{"title":"","slug":"dp","date":"2025-03-08T12:35:51.233Z","updated":"2025-03-08T12:35:53.996Z","comments":true,"path":"2025/03/08/dp/","permalink":"https://wizardtot.github.io/2025/03/08/dp/","excerpt":"","text":"常规思路下最长公共子序列（LCS）的状态转移方程对于一般的最长公共子序列问题，设两个序列分别为 $A &#x3D; [a_1, a_2, \\cdots, a_m]$ 和 $B &#x3D; [b_1, b_2, \\cdots, b_n]$，定义 $dp[i][j]$ 表示 $A$ 的前 $i$ 个元素和 $B$ 的前 $j$ 个元素的最长公共子序列的长度。状态转移方程如下： 当 $a_i &#x3D; b_j$ 时，$dp[i][j] &#x3D; dp[i - 1][j - 1] + 1$。这是因为当当前两个元素相等时，它们可以作为公共子序列的一部分，所以最长公共子序列的长度在前一个状态的基础上加 1。 当 $a_i \\neq b_j$ 时，$dp[i][j] &#x3D; \\max(dp[i - 1][j], dp[i][j - 1])$。也就是说，此时最长公共子序列的长度取 $A$ 的前 $i - 1$ 个元素和 $B$ 的前 $j$ 个元素的最长公共子序列长度，以及 $A$ 的前 $i$ 个元素和 $B$ 的前 $j - 1$ 个元素的最长公共子序列长度中的较大值。 用代码实现上述思路的伪代码如下： 123456789# 初始化 dp 数组dp = [[0] * (n + 1) for _ in range(m + 1)]for i in range(1, m + 1): for j in range(1, n + 1): if A[i - 1] == B[j - 1]: dp[i][j] = dp[i - 1][j - 1] + 1 else: dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])# 最终结果为 dp[m][n] 但是这种方法的时间复杂度是 $O(mn)$，对于本题 $n \\leq 10^5$ 的数据范围会超时。 本题特殊情况的优化思路及状态转移方程本题中两个序列是 $1, 2, \\cdots, n$ 的排列，利用这个特殊性质可以将问题转化为最长上升子序列（LIS）问题。 设两个排列分别为 $P_1$ 和 $P_2$，我们构建一个映射数组 pos，pos[x] 表示 $x$ 在 $P_1$ 中的位置。然后对于 $P_2$ 中的每个元素 $x$，我们将其替换为 pos[x]，得到一个新的序列 $C$。那么 $P_1$ 和 $P_2$ 的最长公共子序列的长度就等于序列 $C$ 的最长上升子序列的长度。 对于最长上升子序列问题，设 $dp[i]$ 表示以第 $i$ 个元素结尾的最长上升子序列的长度，状态转移方程如下： $dp[i] &#x3D; \\max{dp[j] + 1 | j &lt; i, C[j] &lt; C[i]}$ 最终的最长上升子序列长度为 $\\max{dp[i] | 1 \\leq i \\leq n}$。 为了进一步优化时间复杂度，我们可以使用二分查找的方法来维护一个递增的数组 tail，tail[k] 表示长度为 $k + 1$ 的上升子序列的末尾元素的最小值。对于每个元素 $C[i]$，我们在 tail 中二分查找第一个大于等于 $C[i]$ 的位置 $pos$，如果 $pos$ 等于 tail 的长度，说明 $C[i]$ 比 tail 中的所有元素都大，我们可以将其添加到 tail 的末尾；否则，我们将 tail[pos] 更新为 $C[i]$。最终 tail 的长度就是最长上升子序列的长度。 以下是优化后的代码实现： 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main() &#123; int n; cin &gt;&gt; n; vector&lt;int&gt; P1(n + 1), P2(n + 1); vector&lt;int&gt; pos(n + 1); for (int i = 1; i &lt;= n; ++i) &#123; cin &gt;&gt; P1[i]; pos[P1[i]] = i; &#125; for (int i = 1; i &lt;= n; ++i) &#123; cin &gt;&gt; P2[i]; &#125; vector&lt;int&gt; C; for (int i = 1; i &lt;= n; ++i) &#123; C.push_back(pos[P2[i]]); &#125; vector&lt;int&gt; tail; for (int num : C) &#123; auto it = lower_bound(tail.begin(), tail.end(), num); if (it == tail.end()) &#123; tail.push_back(num); &#125; else &#123; *it = num; &#125; &#125; cout &lt;&lt; tail.size() &lt;&lt; endl; return 0;&#125; 这种优化后的方法时间复杂度为 $O(n \\log n)$，可以通过本题的数据范围。","categories":[],"tags":[]},{"title":"TinyKV project 2b and project 2c","slug":"TinyKV中场总结2","date":"2025-02-26T20:44:44.000Z","updated":"2025-05-31T08:15:19.015Z","comments":true,"path":"2025/02/27/TinyKV中场总结2/","permalink":"https://wizardtot.github.io/2025/02/27/TinyKV%E4%B8%AD%E5%9C%BA%E6%80%BB%E7%BB%932/","excerpt":"这是 TinyKV 的 project 2b 和 project 2c 总结贴，思绪似乎有些乱，以后再改改吧","text":"这是 TinyKV 的 project 2b 和 project 2c 总结贴，思绪似乎有些乱，以后再改改吧 fault-tolerant kv storage serviceTinyKV 处理流程在阅读前，回忆tinyKV中 raft 的设计概念，tinyKV 中每个 node 对应一个 raftStore，raftStore 按 key 划分一个节点中的 store 为不同的 peer，一定范围内的 key 的 peer 属于同一个 region．多个节点复制 region 以获得容错性，相同的 region 构成 raft group ，所有的 raft 操作都会在同一个 group 中进行． TinyKV 的处理流程如下： TinyKV Server 通过 gRPC 接口接收客户端的请求req，是系统的 kv 服务入口． raftStorage 在raft_server 中，是 Raft 状态机与 kv 服务之间的中间层．实现了 Storage 接口，在接口中将请求封装成 RaftCmdRequest．在 raftRouter 中将请求发送到对应的peer． raftStore 是 raft 的核心部分．包含两个需要关注的部分．1.raftWorker：轮询处理来自RaftStorage的请求，驱动Raft状态机处理日志复制、心跳等逻辑．2.peerStorage 管理持久化元数据（如RaftLocalState、RaftApplyState），负责Raft日志应用和状态持久化存储（通过raftdb和kvdb）． raftRawNode 接收 peerStorage 的操作以及 step，返回 ready. peerStorage 将 raft 相关的数据持久化，转发 raft 节点的msgs，并将已提交的日志进行应用。peer 即可调用 raftRawNode 的 advance() 更新 raft 的状态。 在peerStorage 中 RaftCmdRequest 后的结果，将封装成 RaftCmdResponse 结构体返回给 RaftStorage 通过以上流程，我们就构建了一个基本的容错 kv 服务。 Server在 server.go 可以看到一个 TinyKV 的服务器存储部分可以实例化为 RaftStorage 或者 StandAloneStorage，当一个请求到达 server 时，server会进一步封装传送到 RaftStorage．具体的设计将在 project 4B 中完成． 1234567type Server struct &#123; storage storage.Storage // (Used in 4B) Latches *latches.Latches // coprocessor API handler, out of course scope copHandler *coprocessor.CopHandler&#125; RaftStorage单节点的存储在 project 1 中已经实现好了．根据指导书指引我们主要还是看懂 kv&#x2F;storage&#x2F;raft_storage&#x2F;raft_server.go​中的RaftStorage． 1234567891011121314type RaftStorage struct &#123; engines *engine_util.Engines // project 1中已解释 config *config.Config // 基本的设置，db的路径以及 raft 基本配置 node *raftstore.Node snapManager *snap.SnapManager // 管理快照的生成和应用 raftRouter *raftstore.RaftstoreRouter // 消息路由 raftSystem *raftstore.Raftstore resolveWorker *worker.Worker snapWorker *worker.Worker // 异步快照处理(2c解释) wg sync.WaitGroup&#125; RaftstoreRouter 负责传递消息给 peer，Server传递给 RaftStorage 的消息分为 kv 类和 raft 类，对应 RaftCmdRequest 以及 RaftMessage，RaftCmdRequest 类型的消息封装了一条可能是读写或者 admin 指令，对应的是kv存储，将消息送到 peerSender 管道． 而 RaftMessage 类型的消息封装了 raft 消息，对应的是 raft 存储，会送到 peerSender 管道，送至 req 目标 region 的 peer．如果 peer 不存在将消息转为 StoreRaftMessage 转移到 storgeSender 管道，在store 级别处理（例如一个region 不存在，需要创建并初始化 region）． 123456789101112131415161718192021222324type router struct &#123; peers sync.Map // regionID -&gt; peerState peerSender chan message.Msg storeSender chan&lt;- message.Msg&#125;func (r *RaftstoreRouter) SendRaftMessage(msg *raft_serverpb.RaftMessage) error &#123; regionID := msg.RegionId if r.router.send(regionID, message.NewPeerMsg(message.MsgTypeRaftMessage, regionID, msg)) != nil &#123; r.router.sendStore(message.NewPeerMsg(message.MsgTypeStoreRaftMessage, regionID, msg)) &#125; return nil&#125;func (r *RaftstoreRouter) SendRaftCommand(req *raft_cmdpb.RaftCmdRequest, cb *message.Callback) error &#123; cmd := &amp;message.MsgRaftCmd&#123; Request: req, Callback: cb, &#125; regionID := req.Header.RegionId return r.router.send(regionID, message.NewPeerMsg(message.MsgTypeRaftCmd, regionID, cmd))&#125; noderaftStore.node 即前文中提到的 node，是节点，在 RaftStorage.Start() 函数中初始化，system 指针指向的 RaftStore 即是节点的 raft 核心状态机．两层 Start() 函数调用后，在 startNode() 中 raft 状态机 node.system 才开始运行． 12345678910111213141516171819202122232425262728293031323334// raft_server.gofunc (rs *RaftStorage) Start() error &#123; //... 其他成员初始化 rs.node = raftstore.NewNode(rs.raftSystem, rs.config, schedulerClient) err = rs.node.Start(context.TODO(), rs.engines, trans, rs.snapManager) if err != nil &#123; return err &#125; return nil&#125;type Node struct &#123; clusterID uint64 // 所属集群标识 store *metapb.Store // 存储节点的元数据 cfg *config.Config // 节点配置参数，存储路径等 system *Raftstore // 核心状态机 schedulerClient scheduler_client.Client&#125;// 紧接着看 node.go 中的 Node.Start() 函数func (n *Node) Start(ctx context.Context, engines *engine_util.Engines, trans Transport, snapMgr *snap.SnapManager) error &#123; // ... 其他成员检查以及初始化 if err = n.startNode(engines, trans, snapMgr); err != nil &#123; return err &#125; return nil&#125;// 最后是 node.go 中的 startNode() 函数func (n *Node) startNode(engines *engine_util.Engines, trans Transport, snapMgr *snap.SnapManager) error &#123; log.Infof(&quot;start raft store node, storeID: %d&quot;, n.store.GetId()) return n.system.start(n.store, n.cfg, engines, trans, n.schedulerClient, snapMgr)&#125; raftStore经过上面层层调用后来到指导书中重点阅读的文件 raftstore.go , 这是整个 tinyKV 最核心的部分．以下将讲述结合TiKV官网 https://cn.pingcap.com/blog/the-design-and-implementation-of-multi-raft/#raftstore 的源码的理解． 在 raftStore 中实现了 multi-raft, multi-raft的结构先前提到过. 一个 Region 内的 peer 支持一定范围的 key, 使用 range 对 key 进行划分, start_key, end_key：用来表示这个 Region 的范围 [start_key, end_key),region_epoch：当一个 Region 添加或者删除 Peer，或者 split 等，我们就会认为这个 Region 的 epoch 发生的变化,peers：当前 Region 包含的节点信息. 在 raftStore 的 storeMeta 的元数据中可以看到, regionRanges 按 Region 的 StartKey 将 Region 有序存储，支持高效的范围查询. 12345678910111213141516171819message Region &#123; uint64 id = 1; // Region key range [start_key, end_key). bytes start_key = 2; bytes end_key = 3; RegionEpoch region_epoch = 4; repeated Peer peers = 5;&#125;type storeMeta struct &#123; sync.RWMutex /// region start key -&gt; region regionRanges *btree.BTree /// region_id -&gt; region regions map[uint64]*metapb.Region /// `MsgRequestVote` messages from newly split Regions shouldn&#x27;t be dropped if there is no /// such Region in this store now. So the messages are recorded temporarily and will be handled later. pendingVotes []*rspb.RaftMessage&#125; raftStore 的结构有些复杂, 在 start() 中主要完成: 启动一些 worker 线程来异步处理特定的任务, 创建GlobalContext, 通过 bs.loadPeers() 函数从持久化存储中加载所有仍存活的 region 并创建 peer 注册到 router 中. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960type Raftstore struct &#123; ctx *GlobalContext storeState *storeState router *router workers *workers tickDriver *tickDriver closeCh chan struct&#123;&#125; wg *sync.WaitGroup&#125;func (bs *Raftstore) start( meta *metapb.Store, cfg *config.Config, engines *engine_util.Engines, trans Transport, schedulerClient scheduler_client.Client, snapMgr *snap.SnapManager) error &#123; y.Assert(bs.workers == nil) // TODO: we can get cluster meta regularly too later. if err := cfg.Validate(); err != nil &#123; return err &#125; err := snapMgr.Init() if err != nil &#123; return err &#125; wg := new(sync.WaitGroup) bs.workers = &amp;workers&#123; splitCheckWorker: worker.NewWorker(&quot;split-check&quot;, wg), regionWorker: worker.NewWorker(&quot;snapshot-worker&quot;, wg), raftLogGCWorker: worker.NewWorker(&quot;raft-gc-worker&quot;, wg), schedulerWorker: worker.NewWorker(&quot;scheduler-worker&quot;, wg), wg: wg, &#125; bs.ctx = &amp;GlobalContext&#123; cfg: cfg, engine: engines, store: meta, storeMeta: newStoreMeta(), snapMgr: snapMgr, router: bs.router, trans: trans, schedulerTaskSender: bs.workers.schedulerWorker.Sender(), regionTaskSender: bs.workers.regionWorker.Sender(), splitCheckTaskSender: bs.workers.splitCheckWorker.Sender(), raftLogGCTaskSender: bs.workers.raftLogGCWorker.Sender(), schedulerClient: schedulerClient, tickDriverSender: bs.tickDriver.newRegionCh, &#125; regionPeers, err := bs.loadPeers() if err != nil &#123; return err &#125; for _, peer := range regionPeers &#123; bs.router.register(peer) &#125; bs.startWorkers(regionPeers) return nil&#125; 上述很多过程在 project 2 中未使用, 直接看 startWorkers() 函数. 该函数创建 raftWorker 和 storeWorker 这两个核心的工作线程, 负责不同的工作, 前文中提到的 raftRouter 路由通过 r.router.send() 将 RaftCmdRequest 和 RaftMessage 传递给对应 regionID 的 peer, 由 raftWorker 来完成处理,同时还要处理来自底层 rawnode 传递的 ready, StoreWorker 管理节点全局的配置, 大致上负责处理 StoreRaftMessage 等消息, 接收来自其他节点的消息,转发或创建 peer 等等. 12345678910111213141516171819202122func (bs *Raftstore) startWorkers(peers []*peer) &#123; ctx := bs.ctx workers := bs.workers router := bs.router bs.wg.Add(2) // raftWorker, storeWorker rw := newRaftWorker(ctx, router) go rw.run(bs.closeCh, bs.wg) sw := newStoreWorker(ctx, bs.storeState) go sw.run(bs.closeCh, bs.wg) router.sendStore(message.Msg&#123;Type: message.MsgTypeStoreStart, Data: ctx.store&#125;) for i := 0; i &lt; len(peers); i++ &#123; regionID := peers[i].regionId _ = router.send(regionID, message.Msg&#123;RegionID: regionID, Type: message.MsgTypeStart&#125;) &#125; engines := ctx.engine cfg := ctx.cfg workers.splitCheckWorker.Start(runner.NewSplitCheckHandler(engines.Kv, NewRaftstoreRouter(router), cfg)) workers.regionWorker.Start(runner.NewRegionTaskHandler(engines, ctx.snapMgr)) workers.raftLogGCWorker.Start(runner.NewRaftLogGCTaskHandler()) workers.schedulerWorker.Start(runner.NewSchedulerTaskHandler(ctx.store.Id, ctx.schedulerClient, NewRaftstoreRouter(router))) go bs.tickDriver.run()&#125; raftWorker 通过管道接收来自 raftStore 的消息(先前提到的 RaftCmdRequest), 以及来自其他节点的 Raft 消息, RaftWorker 从 raftCh 中取得消息, 存入 msgs 数组中, 随后依次处理, 通过当前的PeerState生成PeerMsgHandler​来HandleMsg​。从 Raft 模块获取并处理 ready，处理流程包括转发 raft 消息、持久化状态、将提交的条目应用于状态机。应用后，还要将响应通过回调返回给客户端。 12345678910111213141516171819202122232425262728func (rw *raftWorker) run(closeCh &lt;-chan struct&#123;&#125;, wg *sync.WaitGroup) &#123; defer wg.Done() var msgs []message.Msg for &#123; msgs = msgs[:0] select &#123; case &lt;-closeCh: return case msg := &lt;-rw.raftCh: msgs = append(msgs, msg) &#125; pending := len(rw.raftCh) for i := 0; i &lt; pending; i++ &#123; msgs = append(msgs, &lt;-rw.raftCh) &#125; peerStateMap := make(map[uint64]*peerState) for _, msg := range msgs &#123; peerState := rw.getPeerState(peerStateMap, msg.RegionID) if peerState == nil &#123; continue &#125; newPeerMsgHandler(peerState.peer, rw.ctx).HandleMsg(msg) &#125; for _, peerState := range peerStateMap &#123; newPeerMsgHandler(peerState.peer, rw.ctx).HandleRaftReady() &#125; &#125;&#125; Peer现在终于来到 peer 部分, peer 和 我们先前实现的 raft 以及 rawnode 就是一对一的关系了 先来看看 peer 结构体, 包含 ticker 负责调用 raft.tick(), RaftGroup 即 raft 实例, Meta 对应的是 peer 的元数据, regionId 对应 peer 所处的 region, proposal 数组记录每条 proposal 返回的 callback, LastCompactedIdx 记录上一次压缩的日志索引. 12345678910111213141516171819202122type peer struct &#123; ticker *ticker // Instance of the Raft module RaftGroup *raft.RawNode // The peer storage for the Raft module peerStorage *PeerStorage // Record the meta information of the peer Meta *metapb.Peer regionId uint64 // Tag which is useful for printing log Tag string // Record the callback of the proposals // (Used in 2B) proposals []*proposal // Index of last scheduled compacted raft log. // (Used in 2C) LastCompactedIdx uint64&#125; 来自上层的消息, 层层封装成 MsgRaftCmd 发送到对应的 peer 节点, 在proposeRaftCommand()函数中,peer 节点收到请求后会将指令转换为 entry 发送给整个 raft group 中完成共识, 然后节点在 proposals 后增添一个新的 proposal 等待 callback, 注意这里的 proposal.cb 也就是 callback 是空的, 在 HandleRaftReady 中处理. 12345678910111213141516171819202122232425func (d *peerMsgHandler) HandleMsg(msg message.Msg) &#123; switch msg.Type &#123; case message.MsgTypeRaftMessage: raftMsg := msg.Data.(*rspb.RaftMessage) if err := d.onRaftMsg(raftMsg); err != nil &#123; log.Errorf(&quot;%s handle raft message error %v&quot;, d.Tag, err) &#125; case message.MsgTypeRaftCmd: raftCMD := msg.Data.(*message.MsgRaftCmd) d.proposeRaftCommand(raftCMD.Request, raftCMD.Callback) case message.MsgTypeTick: d.onTick() case message.MsgTypeSplitRegion: split := msg.Data.(*message.MsgSplitRegion) log.Infof(&quot;%s on split with %v&quot;, d.Tag, split.SplitKey) d.onPrepareSplitRegion(split.RegionEpoch, split.SplitKey, split.Callback) case message.MsgTypeRegionApproximateSize: d.onApproximateRegionSize(msg.Data.(uint64)) case message.MsgTypeGcSnap: gcSnap := msg.Data.(*message.MsgGCSnap) d.onGCSnap(gcSnap.Snaps) case message.MsgTypeStart: d.startTicker() &#125;&#125; 当日志条目被 raft 层完成共识, 即 committed 后, 在 peer 如何进一步将得知完成了共识并将已提交的日志进行应用? 这时需要调用 rawnode 中的 haveReady() 函数, 如果为真, 说明更新了 raft 的状态, 在调用 ready() 函数获得 rawnode 的相关更新, 随后即可应用指令. 应用 ready 的实现主要在 HandleRaftReady() 函数中, 这个函数所做的就是调用 SaveReadyState 将 ready 部分的数据进行持久化, 将日志条目写入, 并更新 peerStorage 的 HardState 和 raftState, 应用 snapshot 等. 然后将 raft 节点要转发给其他节点的消息转发出去, 最后依次处理已提交的条目. 完成应用后就要对 proposal 进行处理，将处理的结果封装成 RaftCmdResponse，调用 d.proposals[0].cb.Done(resp) 将结果通过管道返回至 raftStorage，对于写的请求只需检查即可，对于读请求返回对应的值。 以上粗略记录了整个 TinyKV 服务的调用链，可以看到需要实现的内容并不多。 project 2bPeerStoragepeerStorage 中有三个状态： RaftLocalState：用于存储当前 Raft 的 HardState 和最后一个 Log 的 Index。 RaftApplyState：用于存储 Raft 应用的最后一个 Log 索引和一些截断的 Log 信息。 RegionLocalState：用于存储此 Store 上的 Region 信息和相应的 Peer State。Normal 表示该 Peer 正常，Tombstone 表示该 Peer 已从 Region 中移除，无法加入 Raft Group。 这些状态存储在两个 badger 实例中：raftDB 和 kvDB： raftdb 存储 Raft 日志和RaftLocalState​ kvdb 将键值数据存储在不同的列族，RegionLocalState​和RaftApplyState​中。你可以把 kvdb 看作是 Raft 论文中提到的状态机。 涉及到上述两个 badger 数据的写入和状态的更新都要用 writeBatch.SetMeta() 进行批量写入。 我们首先要实现 peer_storage.go 中的 Append() 函数，就是将给定的 entries 写入到 raftDB 进行持久化。根据指导书，我们要做的： 将 entries 低于 ps.FirstIndex() 的部分进行截断。 遍历 entries 通过 raftWB.SetMeta() 将条目放入 WriteBatch 删除之前追加的条目，这些条目永远不会提交，通过 raftWB.DeleteMeta() 将条目删除。 随后还要实现 SaveReadyState() 函数(提前布置些 2c 内容)，涉及对 raftDB 和 kvDB的修改，要两个 WriteBatch 函数，做以下工作： 若 ready 中的快照不为空，调用 ps.ApplySnapshot() 将快照应用 调用 ps.Append() 将 ready 中未应用的日志应用 更新 raftState，即更新 LastIndex 和 LastTerm，通过 SetMeta() 放入 WriteBatch 最后一次写入 raftDB 和 kvDB12345678910111213141516171819202122232425262728func (ps *PeerStorage) SaveReadyState(ready *raft.Ready) (*ApplySnapResult, error) &#123; // Hint: you may call `Append()` and `ApplySnapshot()` in this function // Your Code Here (2B/2C). var applySnapResult *ApplySnapResult var err error kvWB := new(engine_util.WriteBatch) raftWB := new(engine_util.WriteBatch) if !raft.IsEmptySnap(&amp;ready.Snapshot) &#123; applySnapResult, err = ps.ApplySnapshot(&amp;ready.Snapshot, kvWB, raftWB) if err != nil &#123; return nil, err &#125; &#125; err = ps.Append(ready.Entries, raftWB) if len(ready.Entries) &gt; 0 &#123; newLastIndex := ready.Entries[len(ready.Entries)-1].Index newLastTerm := ready.Entries[len(ready.Entries)-1].Term ps.raftState.LastIndex = newLastIndex ps.raftState.LastTerm = newLastTerm &#125; if !raft.IsEmptyHardState(ready.HardState) &#123; ps.raftState.HardState = &amp;ready.HardState &#125; err = raftWB.SetMeta(meta.RaftStateKey(ps.region.Id), ps.raftState) err = raftWB.WriteToDB(ps.Engines.Raft) err = kvWB.WriteToDB(ps.Engines.Kv) return applySnapResult, err&#125; PeerMsgHandler在 peer_msg_handler.go 函数中要实现前文中提到的两个核心的函数。首先是 proposeRaftCommand(), 根据指导书，要处理没有发送给 leader 和 日志被覆盖 这两个错误，已经在 preProposeRaftCommand() 执行好了，将错误返回给 cb 即可。 遍历 msg.Requests , 获取请求操作的 key，如果 key 不属于当前的 region，将错误返回给 cb。随后调用 d.RaftGroup.Propose() 将请求推送给 raft 集群完成共识。创建新的 propose 等待 callback。 123456789101112131415161718192021222324252627282930313233343536373839func (d *peerMsgHandler) proposeRaftCommand(msg *raft_cmdpb.RaftCmdRequest, cb *message.Callback) &#123; err := d.preProposeRaftCommand(msg) if err != nil &#123; cb.Done(ErrResp(err)) return &#125; // Your Code Here (2B). for _, req := range msg.Requests &#123; var key []byte switch req.CmdType &#123; case raft_cmdpb.CmdType_Get: key = req.Get.Key case raft_cmdpb.CmdType_Put: key = req.Put.Key case raft_cmdpb.CmdType_Delete: key = req.Delete.Key &#125; err := util.CheckKeyInRegion(key, d.Region()) if err != nil &amp;&amp; req.CmdType != raft_cmdpb.CmdType_Snap &#123; cb.Done(ErrResp(err)) continue &#125; data, err1 := msg.Marshal() if err1 != nil &#123; log.Panic(err1) &#125; newProposal := &amp;proposal&#123; index: d.nextProposalIndex(), term: d.Term(), cb: cb, &#125; err = d.RaftGroup.Propose(data) if err != nil &#123; cb.Done(ErrResp(err)) return &#125; d.proposals = append(d.proposals, newProposal) &#125;&#125; handleRaftReady() 之前提到过工作流程，调用 SaveReadyState() 对 ready 中部分数据进行持久化，随后将 raft 节点的消息转发出去，重点是实现应用 ready.CommittedEntries 逻辑。应用已提交的日志主要针对 kvDB 的修改。当然需要创建 WriteBatch。 基本上就是遍历entry中 msg.Requests 执行相应操作，直接看代码： 1234567891011for _, req := range msg.Requests &#123; switch req.CmdType &#123; case raft_cmdpb.CmdType_Get: //读和snap不着急处理 case raft_cmdpb.CmdType_Put: kvWB.SetCF(req.Put.Cf, req.Put.Key, req.Put.Value) case raft_cmdpb.CmdType_Delete: kvWB.DeleteCF(req.Delete.Cf, req.Delete.Key) case raft_cmdpb.CmdType_Snap: &#125; d.ProposeCallBack(entry, req, nil) &#125; 随后处理这个 request 对应的 proposal，可能有些 proposal 的 index 低于 entry 的 index，那么这些 proposal 就是过时的，需要提出。如过 proposal 的 term 不一致说明 request 过时了。最后调用 cb.Done() 将 callback 返回到上层即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (d *peerMsgHandler) dropStaleProposals(entry *eraftpb.Entry) &#123; i := 0 for ; i &lt; len(d.proposals); i++ &#123; if d.proposals[i].index &lt; entry.Index &#123; d.proposals[i].cb.Done(ErrResp(&amp;util.ErrStaleCommand&#123;&#125;)) &#125; else &#123; break &#125; &#125; d.proposals = d.proposals[i:]&#125;func (d *peerMsgHandler) ProposeCallBack(entry *eraftpb.Entry, req *raft_cmdpb.Request, resp *raft_cmdpb.RaftCmdResponse) &#123; d.dropStaleProposals(entry) if len(d.proposals) &gt; 0 &#123; if d.proposals[0].index != entry.Index &#123; return &#125; if d.proposals[0].term != entry.Term &#123; NotifyStaleReq(entry.Term, d.proposals[0].cb) d.proposals = d.proposals[1:] return &#125; if resp == nil &#123; resp = &amp;raft_cmdpb.RaftCmdResponse&#123; Header: &amp;raft_cmdpb.RaftResponseHeader&#123;&#125;, Responses: nil, AdminResponse: nil, &#125; &#125; switch req.CmdType &#123; case raft_cmdpb.CmdType_Get: val, _ := engine_util.GetCF(d.peerStorage.Engines.Kv, req.Get.Cf, req.Get.Key) resp.Responses = []*raft_cmdpb.Response&#123;&#123;CmdType: raft_cmdpb.CmdType_Get, Get: &amp;raft_cmdpb.GetResponse&#123;Value: val&#125;&#125;&#125; case raft_cmdpb.CmdType_Put: resp.Responses = []*raft_cmdpb.Response&#123;&#123;CmdType: raft_cmdpb.CmdType_Put, Put: &amp;raft_cmdpb.PutResponse&#123;&#125;&#125;&#125; case raft_cmdpb.CmdType_Delete: resp.Responses = []*raft_cmdpb.Response&#123;&#123;CmdType: raft_cmdpb.CmdType_Delete, Delete: &amp;raft_cmdpb.DeleteResponse&#123;&#125;&#125;&#125; case raft_cmdpb.CmdType_Snap: resp.Responses = []*raft_cmdpb.Response&#123;&#123;CmdType: raft_cmdpb.CmdType_Snap, Snap: &amp;raft_cmdpb.SnapResponse&#123;Region: d.Region()&#125;&#125;&#125; d.proposals[0].cb.Txn = d.peerStorage.Engines.Kv.NewTransaction(false) &#125; d.proposals[0].cb.Done(resp) d.proposals = d.proposals[1:] &#125;&#125; 当然除了写入 kv 之外，还要更改 applyState, 应用日志后将 applyIndex 更新。 project 2c前面的 project 实现了基本的容错 kv 服务，每个 raft 的日志是会无线增长的，我们要在此基础上实现日志压缩和快照。与 6.824 不同，TinyKV 的快照和压缩日志是相对独立的，leader 定期压缩日志，不生成快照，然后 follower 的日志落后时才会生成快照生成交给 follower 去应用。 raft 层次快照当 leader 在复制日志时发现某个 follower 的日志十分落后，即 prevIndex 低于 leader 日志中的 firstIndex 时，就要生成并发送日志。如果 leader 有待应用的 pendingSnapshot 即发送 pendingSnapshot，否则生成 Snapshot，生成 Snapshot 的函数已经实现好，直接调用 storage.Snapshot() 即可。将 Snapshot 发送给对应的 follower。 在 peerStorage 中可以看到，生成 SnapShot 的过程并不简单。要将 ps 的 snapState 更改为 SnapState_Generating，并生成一个 RegionTaskGen 异步生成 SnapShot。如果 SnapShot 没有生成好，就会返回 raft.ErrSnapshotTemporarilyUnavailable ，leader 稍后重试即可。 123456789101112131415161718192021222324252627282930313233343536373839404142func (ps *PeerStorage) Snapshot() (eraftpb.Snapshot, error) &#123; var snapshot eraftpb.Snapshot if ps.snapState.StateType == snap.SnapState_Generating &#123; select &#123; case s := &lt;-ps.snapState.Receiver: if s != nil &#123; snapshot = *s &#125; default: return snapshot, raft.ErrSnapshotTemporarilyUnavailable &#125; ps.snapState.StateType = snap.SnapState_Relax if snapshot.GetMetadata() != nil &#123; ps.snapTriedCnt = 0 if ps.validateSnap(&amp;snapshot) &#123; return snapshot, nil &#125; &#125; else &#123; log.Warnf(&quot;%s failed to try generating snapshot, times: %d&quot;, ps.Tag, ps.snapTriedCnt) &#125; &#125; if ps.snapTriedCnt &gt;= 5 &#123; err := errors.Errorf(&quot;failed to get snapshot after %d times&quot;, ps.snapTriedCnt) ps.snapTriedCnt = 0 return snapshot, err &#125; log.Infof(&quot;%s requesting snapshot&quot;, ps.Tag) ps.snapTriedCnt++ ch := make(chan *eraftpb.Snapshot, 1) ps.snapState = snap.SnapState&#123; StateType: snap.SnapState_Generating, Receiver: ch, &#125; // schedule snapshot generate task ps.regionSched &lt;- &amp;runner.RegionTaskGen&#123; RegionId: ps.region.GetId(), Notifier: ch, &#125; return snapshot, raft.ErrSnapshotTemporarilyUnavailable&#125; 在 raft 中 follower 接收到 snapshot 后，会根据 snapshot 中的元数据更新 confState，将 snapIndex 之前的日志进行截断。将 pendingSnapshot 更改为 snapshot 并等待上层的 handleRaftReady() 进行应用。 Peer 层次快照前文提到的仅仅是 follower 被动接收 leader 生成的快照，作为 leader 还需要主动压缩日志。主动生成快照的判断逻辑在 peer_msg_handler.go 中已经实现好了，ticker 定期调用 onRaftGCLogTick() 检查是否满足 appliedIdx-firstIdx &gt;&#x3D; d.ctx.cfg.RaftLogGcCountLimit，满足条件创造一个压缩日志的 request，这个 request 的类型为 dminCmdType_CompactLog。 12345678910111213141516171819202122232425262728293031323334func (d *peerMsgHandler) onRaftGCLogTick() &#123; d.ticker.schedule(PeerTickRaftLogGC) if !d.IsLeader() &#123; return &#125; appliedIdx := d.peerStorage.AppliedIndex() firstIdx, _ := d.peerStorage.FirstIndex() var compactIdx uint64 if appliedIdx &gt; firstIdx &amp;&amp; appliedIdx-firstIdx &gt;= d.ctx.cfg.RaftLogGcCountLimit &#123; compactIdx = appliedIdx &#125; else &#123; return &#125; y.Assert(compactIdx &gt; 0) compactIdx -= 1 if compactIdx &lt; firstIdx &#123; // In case compact_idx == first_idx before subtraction. return &#125; term, err := d.RaftGroup.Raft.RaftLog.Term(compactIdx) if err != nil &#123; log.Fatalf(&quot;appliedIdx: %d, firstIdx: %d, compactIdx: %d&quot;, appliedIdx, firstIdx, compactIdx) panic(err) &#125; // Create a compact log request and notify directly. regionID := d.regionId request := newCompactLogRequest(regionID, d.Meta, compactIdx, term) d.proposeRaftCommand(request, nil)&#125; 调用 d.proposeRaftCommand(request, nil) 后，这个 request 会成为一个 entry 交给 raft group 进行共识，共识完成后在 HandleRaftReady() 中，我们要应用这个日志压缩。要做的仅需更新 applyState 然后调用 ScheduleCompactLog() 生成一个 RaftLogGCTask。在 gcRaftLog() 函数中对 raftDB 持久化的部分 entries 进行删除。 123456789101112func (d *peerMsgHandler) CompactLog(entry *eraftpb.Entry, kvWB *engine_util.WriteBatch, req *raft_cmdpb.AdminRequest) &#123; compactLog := req.GetCompactLog() if compactLog.CompactIndex &gt; d.peerStorage.applyState.TruncatedState.Index &#123; d.peerStorage.applyState.TruncatedState.Index = compactLog.CompactIndex d.peerStorage.applyState.TruncatedState.Term = compactLog.CompactTerm if err := kvWB.SetMeta(meta.ApplyStateKey(d.Region().GetId()), d.peerStorage.applyState); err != nil &#123; log.Panic(err) return &#125; &#125; d.ScheduleCompactLog(compactLog.CompactIndex)&#125; 不同与 6.824 的 raft，完成压缩后节点并不会自主生成一个 snapshot 准备发送给其他的节点。压缩日志后 FirstIndex() 返回的是 applyState.TruncatedState.Index + 1，即压缩后的第一条日志的索引。之后当 follower 落后于这个 index 时，会像前文中提到那样异步生成 snapshot 并发送。 在前文中提到，在 PeerStorage.SaveReadyState() 中发现 ready 中快照不为空时需要进行应用。应用快照与生成快照一样，需要创建异步的任务 RegionTaskApply。在 ApplySnapshot 中首先要通过 ps.clearMeta() 和 ps.clearExtraData() 来清除旧的数据，要更改 raftState 和 applyState，将 snapState 改为 SnapState_Applying。并且创建 RegionTaskApply，传递给 ps.regionSched，最后返回 applySnapResult。 12345678910111213141516ch := make(chan bool, 1) ps.regionSched &lt;- runner.RegionTaskApply&#123; RegionId: snapData.GetRegion().GetId(), Notifier: ch, SnapMeta: snapshot.Metadata, StartKey: snapData.GetRegion().GetStartKey(), EndKey: snapData.GetRegion().GetEndKey(), &#125; if !(&lt;-ch) &#123; return nil, nil &#125; applySnapResult := &amp;ApplySnapResult&#123; PrevRegion: ps.region, Region: snapData.GetRegion(), &#125; return applySnapResult, nil 在 HandleRaftReady() 中返回 applySnapResult 可能会涉及对 region 的更改： 12345678if !reflect.DeepEqual(applyResult.Region, applyResult.PrevRegion) &#123; d.peerStorage.SetRegion(applyResult.Region) d.ctx.storeMeta.Lock() d.ctx.storeMeta.regions[applyResult.Region.Id] = applyResult.Region d.ctx.storeMeta.regionRanges.Delete(&amp;regionItem&#123;region: applyResult.PrevRegion&#125;) d.ctx.storeMeta.regionRanges.ReplaceOrInsert(&amp;regionItem&#123;region: applyResult.Region&#125;) d.ctx.storeMeta.Unlock() &#125; follower 应用 snapshot 完成后，在 advance() 推进时将 pendingSnapshot 设置为空，至此快照应用结束。","categories":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://wizardtot.github.io/categories/TinyKV/"}],"tags":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://wizardtot.github.io/tags/TinyKV/"}],"author":"WizardTOT"},{"title":"TinyKV project 1 and project 2A","slug":"TinyKV中场总结1","date":"2025-02-23T06:39:12.000Z","updated":"2025-05-31T08:15:08.873Z","comments":true,"path":"2025/02/23/TinyKV中场总结1/","permalink":"https://wizardtot.github.io/2025/02/23/TinyKV%E4%B8%AD%E5%9C%BA%E6%80%BB%E7%BB%931/","excerpt":"这是 TinyKV 的 project 1 和 project 2 总结贴，细致的线性测试很友好 可能有些模糊的地方，当然这些是给自己复习看的","text":"这是 TinyKV 的 project 1 和 project 2 总结贴，细致的线性测试很友好 可能有些模糊的地方，当然这些是给自己复习看的 TinyKV project 原链接：https://github.com/talent-plan/tinykv Project1 StandaloneKV通过 project1 熟悉 TinyKV 的底层。要求基于 bagerDB 实现单节点的存储引擎，并且支持列族(column family)。与后续的 raft 部分并没有太多关联，但可以让我们对底层的存储窥见一二。 Column FamilybagerDB 自身并不支持 CF，普通的 key-value 存储是无法处理关系式事务的，所以通过 CF 的形式，将列名作为键的前缀构成一个新键，一个列构成一个列族。 在 engine_util.go 中可以看出 CF 的存储方式。 1234567891011121314151617181920func KeyWithCF(cf string, key []byte) []byte &#123; return append([]byte(cf+&quot;_&quot;), key...)&#125;func GetCF(db *badger.DB, cf string, key []byte) (val []byte, err error) &#123; err = db.View(func(txn *badger.Txn) error &#123; val, err = GetCFFromTxn(txn, cf, key) return err &#125;) return&#125;func GetCFFromTxn(txn *badger.Txn, cf string, key []byte) (val []byte, err error) &#123; item, err := txn.Get(KeyWithCF(cf, key)) if err != nil &#123; return nil, err &#125; val, err = item.ValueCopy(val) return&#125; Project1A在 1A 中实现 standAloneStorage 的初始化以及以下接口： 12345678910111213type Storage interface &#123; Start() error Stop() error Write(ctx *kvrpcpb.Context, batch []Modify) error Reader(ctx *kvrpcpb.Context) (StorageReader, error)&#125;type StorageReader interface &#123; // When the key doesn&#x27;t exist, return nil for the value GetCF(cf string, key []byte) ([]byte, error) IterCF(cf string) engine_util.DBIterator Close()&#125; 定义 storage 要包含 engine，engine 包含两个 bager 数据库，一个用于存放 kv ，另一个用于存放 raft 日志及信息。同时初始化 storage 还需要路径信息，从 config 参数中获取即可。 1234567type Engines struct &#123; Kv *badger.DB KvPath string // Metadata used by Raft. Raft *badger.DB RaftPath string&#125; 实现 reader 方法时要返回一个 StorageReader 接口，接口中三个函数也要实现，分别代表获取 CF_key 对应的 value，返回一个迭代器，关闭 reader 。 在实现时要开启一个事务，然后调用engine_util 中封装好的函数就行了。 123456789101112func (r *StandAloneStorageReader) GetCF(cf string, key []byte) ([]byte, error) &#123; val, err := engine_util.GetCFFromTxn(r.txn, cf, key) if err == badger.ErrKeyNotFound &#123; return nil, nil &#125; return val, err&#125;func (r *StandAloneStorageReader) IterCF(cf string) engine_util.DBIterator &#123; iter := engine_util.NewCFIterator(cf, r.txn) return iter&#125; 写操作无需事务直接执行，并且是批处理，主要包含 put 和 delete 操作： 1234567891011121314type Modify struct &#123; Data interface&#123;&#125;&#125;type Put struct &#123; Key []byte Value []byte Cf string&#125;type Delete struct &#123; Key []byte Cf string&#125; 遍历 batch，逐一进行 put 和 delete： 1234567891011121314for _, b := range batch &#123; switch b.Data.(type) &#123; case storage.Put: put := b.Data.(storage.Put) if err := engine_util.PutCF(s.engines.Kv, put.Cf, put.Key, put.Value); err != nil &#123; return err &#125; case storage.Delete: del := b.Data.(storage.Delete) if err := engine_util.DeleteCF(s.engines.Kv, del.Cf, del.Key); err != nil &#123; return err &#125; &#125; &#125; Project1B在 storage 基础上实现 raw API，供 project 4 使用。对于 rawGet 和 rawScan ，通过之前设计的 reader 将获得的结果组成 kvPair 回复，注意 rawScan 中定义了 limit 字段限制获得结果的数量。对于 rawPut 和 rawDelete 将操作定义成 modify 结构体接着调用 storage.Write() 执行。 Project2 RaftKV在Project 1中实现了单节点的存储，在这一层，需要实现基于 raft 的kv存储。需要完成三个部分： 实现最基本的 raft 共识算法，并实现 rawnode 构建容错的 kv 服务器 对以上内容增加日志压缩和快照 Project2A Raft共识算法在 Project2A 中，仅需集中于 raft 类，log 日志结构以及raft 类上层的封装 rawnode。处理流程基本如下： 暂不考虑上层应用如何处理的，上层应用将一条指令例如 put,delete,scan…封装成MsgRaftCmd 发送给对应的 peer 节点。peer 节点再将指令封装成 entry，将 entry propose 到其他的节点。通过 RaftMessage 完成传递和共识， 等待节点的 callback 。只有大部分节点接受了这个 entry 才能达成共识即 committed 。 节点会调用 raft 的封装 rawnode 的接口完成共识操作。一个 rawnode 包含 raft 类以及易失的 softState，需要保存的 hardState。 与 6.824 的计时不同，这里的rawnode可以视作为一个状态机，采用逻辑时钟推进，上层应用通过 Raw Node.Tick() 来推动 raft 节点的逻辑时钟，从而触发 leader 心跳和选举。 上层通过 RawNode.Propose() 向 raft 节点发送新的日志条目。 上层通过 RawNode.Step() 将给定的消息推进到 raft 状态机，rawNode 会进一步将消息进行处理。然后上层会通过 RawNode.HasReady() 和 RawNode.Ready() 来获取 raft 节点的更新，从而进行实际的数据操作和持久化等。 通过 RawNode.Advance() 通知 RawNode 应用已经应用并保存了上次 Ready 结果中的进度。它更新内部状态，如稳定日志索引、应用日志索引等。上层应用在处理完 Ready 返回的状态后，调用此方法告知 RawNode 可以进行下一步操作。 日志在 TinyKV 中 raft 日志结构如下，与日志相关的操作需要谨慎，以免出现越界。 123snapshot/first.....applied....committed....stabled.....last// --------|------------------------------------------------|// log entries raftLog 包含一个 storage 结构,用于存放自上次快照以来所有的稳定日志条目。 committed 表示在大多数节点中稳定存储日志的最高位置，当所有的日志条目被大多数确认后，其索引会更新到 committed 中。 applied 表示应用程序已经被指示应用到其状态机的最高日志位置。应用程序会根据 applied 的值来决定从日志中取出哪些条目进行状态更新。 表示已经持久化到存储中的日志条目的最高索引。stabled 用于记录尚未被 storage 持久化的日志，每次处理 Ready 时，会包含这些不稳定的日志。 entries 存放所有尚未压缩的日志条目。 pendingSnapshot 存在表示正在接收的不稳定快照。 在初始化 newLog() 时注意从 storage 中获取第一个索引和最后一个索引，由于 storage 中保存所有的已被持久化但未被应用的条目，所以 applied &#x3D; firstIndex - 1。 common与 6.824 中 raft 的 RPC 调用不同，TinyKV 是通过消息队列的形式完成消息传递和操作，每一个 raft 节点均需要处理的消息类型有(仅在 project 2A 中)： MsgType 描述 对应函数 MsgHup 本地消息，发起选举 startElection() MsgAppend leader向其他节点同步日志的消息 handleAppendEntries() MsgHeartbeat leader发送的心跳 handleHeartbeat() MsgRequestVote candidate发送的投票请求 handleRequestVote() MsgTimeoutNow 目标节点收到该消息，即刻自增 term 发起选举 startElection() tick() 函数用于推进逻辑时钟。本地消息心跳和选举通过tick()来完成。首先对于 follower 和 candidate 有 electionElapsed 来计时，每当 electionElapsed &gt;&#x3D; 随机的超时时间时需要发送 MsgHup 的本地消息触发选举。这里注意每次一个 raft 节点成为 follower 或者 candidate 都需要重新设置超时时间并保持。对于 leader 可通过 electionElapsed 定期检查是否有较多的 follower 支持(可能会出现孤岛或者网络分区的情况)，增加 heartbeatElapsed 时钟检查是否需要发送心跳。 12345678910111213141516171819202122232425262728293031323334353637func (r *Raft) tick() &#123; // Your Code Here (2A). switch r.State &#123; case StateFollower, StateCandidate: r.electionElapsed++ if r.electionElapsed &gt;= r.randomizedElectionTimeout &#123; r.electionElapsed = 0 msg := pb.Message&#123; MsgType: pb.MessageType_MsgHup, To: r.id, From: r.id, &#125; r.Step(msg) &#125; case StateLeader: r.electionElapsed++ hb := len(r.resHeartBeat) if r.electionElapsed &gt;= r.electionTimeout &#123; r.electionElapsed = 0 r.resHeartBeat = make(map[uint64]bool) r.resHeartBeat[r.id] = true if hb*2 &lt;= len(r.Prs) &#123; r.startElection() &#125; &#125; r.heartbeatElapsed++ if r.heartbeatElapsed &gt;= r.heartbeatTimeout &#123; r.heartbeatElapsed = 0 msg := pb.Message&#123; MsgType: pb.MessageType_MsgBeat, To: r.id, From: r.id, &#125; r.Step(msg) &#125; &#125;&#125; step()函数用于处理消息。根据接收到的消息类型与节点相应的状态执行相应的操作。消息分为本地消息和普通消息，对于本地消息是直接发送给本地节点的，仅需记录 to 字段和类型，直接调用 step() 函数执行即可。对于普通消息需要在to字段中记录发送的目标节点以及相关的消息存入 raft 的消息队列 r.msgs 中即可由目标节点调用 step() 完成执行。 startElection()函数用于发起选举，选举时首先要将身份更新为 candidate，并向其余的节点发送 requestVote 消息，要在消息中记录节点最后一条日志条目的index 和 term 便于比较。 12345678910111213func (r *Raft) sendRequestVote(to uint64) &#123; lastIndex := r.RaftLog.LastIndex() lastTerm, _ := r.RaftLog.Term(lastIndex) msg := pb.Message&#123; MsgType: pb.MessageType_MsgRequestVote, To: to, From: r.id, Term: r.Term, LogTerm: lastTerm, Index: lastIndex, &#125; r.msgs = append(r.msgs, msg)&#125; handleRequestVote() 函数用于接收 candidate 发起的投票请求。按照论文中完成判断和更新 vote 即可。 1234567891011121314151617181920212223242526func (r *Raft) handleRequestVote(m pb.Message) &#123; lastIndex := r.RaftLog.LastIndex() lastTerm, _ := r.RaftLog.Term(lastIndex) res := pb.Message&#123; MsgType: pb.MessageType_MsgRequestVoteResponse, To: m.From, From: r.id, Term: r.Term, Reject: true, &#125; if m.Term &lt; r.Term &#123; r.msgs = append(r.msgs, res) return &#125; if m.Term &gt; r.Term &#123; r.becomeFollower(m.Term, None) res.Term = r.Term &#125; if r.Vote == None || r.Vote == m.From &#123; if m.LogTerm &gt; lastTerm || (m.LogTerm == lastTerm &amp;&amp; m.Index &gt;= lastIndex) &#123; r.Vote = m.From res.Reject = false &#125; &#125; r.msgs = append(r.msgs, res)&#125; handleAppendEntries() 函数用于处理 leader 同步日志的消息。节点首先要比较 term 确认 leader 有更高的日期，接着确认 m.Index 是否低于 lastIndex ，否则拒绝消息，并发送 lastIndex 通知 leader 更新 match 和 next，然后再比较节点中 m.Index 的 term 是否一致，将不一致部分截断。 经过上面的检查后，日志的 index 和 term 是匹配的，但仍然会出现部分日志 term 不匹配的情况，这时需要将不匹配之后的日志条目截断然后追加新的日志。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960func (r *Raft) handleAppendEntries(m pb.Message) &#123; // Your Code Here (2A). if m.Term &lt; r.Term &#123; res := pb.Message&#123; MsgType: pb.MessageType_MsgAppendResponse, To: m.From, From: r.id, Term: r.Term, Reject: true, &#125; r.msgs = append(r.msgs, res) return &#125; if m.Term == r.Term &amp;&amp; r.State == StateLeader &#123; return &#125; r.becomeFollower(m.Term, m.From) lastIndex := r.RaftLog.LastIndex() lastTerm, _ := r.RaftLog.Term(lastIndex) if m.Index &gt; lastIndex &#123; res := pb.Message&#123; MsgType: pb.MessageType_MsgAppendResponse, To: m.From, From: r.id, Term: r.Term, LogTerm: lastTerm, Index: lastIndex, Reject: true, &#125; r.msgs = append(r.msgs, res) return &#125; logTerm, _ := r.RaftLog.Term(m.Index) if m.LogTerm != logTerm &#123; res := pb.Message&#123; MsgType: pb.MessageType_MsgAppendResponse, To: m.From, From: r.id, Term: r.Term, LogTerm: logTerm, Index: m.Index, Reject: true, &#125; r.msgs = append(r.msgs, res) return &#125; r.RaftLog.ReplaceEntries(m.Entries) newIndex := m.Index + uint64(len(m.Entries)) r.RaftLog.SetCommit(min(m.Commit, newIndex)) res := pb.Message&#123; MsgType: pb.MessageType_MsgAppendResponse, To: m.From, From: r.id, Term: r.Term, Index: r.RaftLog.LastIndex(), Reject: false, &#125; r.msgs = append(r.msgs, res)&#125; 为何检查到不匹配后才会截断追加日志，而不是直接将 prevIndex 后的日志截断追加？ 考虑到如下情况：假设 index 从0开始，一个 follower 的日志为[1,1,2,2]，一个 leader 的日志为[1,1,2,2,3,3]，commitIndex &#x3D; 3，发送给这个 follower 的日志为[3,3]，prevIndex 为3，commitIndex &#x3D; 3，当这个消息由于某种原因延迟到达，此时 leader 更新为[1,1,2,2,3,3,4,4] 并获得大部分节点的支持，将 commitIndex 设置为 7。 此时这个 follower 将获得[3,3,4,4]，commitIndex &#x3D; 7，追加后，此时 leader点故障，这个follower 成为 leader，接收到先前的消息[3,3]，这条消息时能经过前面两个一致性检查的，若直接截断 prevIndex ，此时节点的日志为[1,1,2,2,3,3]，这个新leader的commitIndex 将会退回5，index 大于 5 的日志全部丢失了。 这并非是想要的结果，仅有日志不一致时才能截断，例如 follower 日志为[1,1,2,2,3,3,4,4],接收到prevIndex 为 3 的条目 [4,4] 或者 [2,2,3,3] 才能截断并存放。 由于 raft 对日志具有一致性保证，相同任期和 index 的日志条目是一致的，可以跳过。 12345678910111213141516func (l *RaftLog) ReplaceEntries(entries []*pb.Entry) &#123; for _, entry := range entries &#123; term, _ := l.Term(entry.Index) if entry.Index-l.FirstIndex() &gt; uint64(len(l.entries)) || entry.Index &gt; l.LastIndex() &#123; l.entries = append(l.entries, *entry) &#125; else if entry.Term != term &#123; if entry.Index &lt; l.FirstIndex() &#123; l.entries = make([]pb.Entry, 0) &#125; else &#123; l.entries = l.entries[:entry.Index-l.FirstIndex()] &#125; l.stabled = min(l.stabled, entry.Index-1) l.entries = append(l.entries, *entry) &#125; &#125;&#125; candidate MsgType 描述 对应函数 MsgRequestVoteResponse 记录投票回复 handleRequestResponse() candidate 每次收到投票后需要重新遍历 r.votes 记录支持数和不支持数，当支持数过半时会成为 leader，反过来会退回 follower。由测试发现退回 follower 要 term 要设置为 m.Term，并且不能直接在 raft 中记录全局的投票数，测试会针对发送重复的投票回复。 1234567891011121314151617181920func (r *Raft) handleRequestResponse(m pb.Message) &#123; agr := 0 dis := 0 r.votes[m.From] = !m.Reject if m.Term &gt; r.Term &#123; r.becomeFollower(m.Term, None) &#125; for _, vote := range r.votes &#123; if vote &#123; agr++ &#125; else &#123; dis++ &#125; &#125; if agr*2 &gt; len(r.Prs) &#123; r.becomeLeader() &#125; else if dis*2 &gt;= len(r.Prs) &#123; r.becomeFollower(m.Term, None) &#125;&#125; leader MsgType 描述 对应函数 MessageType_MsgBeat 本地消息，Leader广播心跳 sendHeartbeat() MsgPropose 本地消息，来自上层应用的Propose请求。只有Leader实际处理 handlePropose() MsgAppendResponse 用于处理其他节点appendEntries和appendSnapshot的回复，leader需要据此更新match和next handleAppendResponse() MsgHeartbeatResponse 心跳响应，包含当前follower的Commit索引 handleHeartBeatResponse() 当节点成为 leader 时要完成基本的初始化，在becomeLeader()中将所有 peer 的 next 更新为 lastIndex + 1。并要上传一条当前任期的空日志，这是对应于论文中提到的 leader 只能 commit 当前 term 的日志，上传空日志后旧任期的日志就会被间接提交。 12345678910111213141516171819202122232425262728func (r *Raft) becomeLeader() &#123; // Your Code Here (2A). // NOTE: Leader should propose a noop entry on its term if r.State == StateLeader &#123; return &#125; r.resHeartBeat = make(map[uint64]bool) r.resHeartBeat[r.id] = true r.State = StateLeader for pr := range r.Prs &#123; r.Prs[pr].Next = r.RaftLog.LastIndex() + 1 r.Prs[pr].Match = 0 &#125; entry := pb.Entry&#123; EntryType: pb.EntryType_EntryNormal, Term: r.Term, Index: r.RaftLog.LastIndex() + 1, Data: nil, &#125; r.RaftLog.AppendEntry(entry) r.Prs[r.id].Match = r.RaftLog.LastIndex() r.Prs[r.id].Next = r.Prs[r.id].Match + 1 for pr := range r.Prs &#123; if pr != r.id &#123; r.sendAppend(pr) &#125; &#125;&#125; handleAppendResponse 快速回退看 6.824 那部分，基本一样的。。。 很重要的问题，leader 何时更新 committed ，更新后的索引该如何告知 follower? 有很多测试针对 committed 的更新，leader 在接收到 appendResponse 后会检查是否更新 committed，leader 更新 committed 为大部分节点都存在的idnex。更新后才能将 committed 通知给其他的 follower，此时需要再发送一个 appendEntries 更新 follower 的 committed。 处理心跳时发现 follower 的 committed 落后也是需要及时更新的，但是不能直接更新因为此时的 follower 并没有同步最新的 entry。leader 需要再发送 appendEntries 完成日志同步并更新committed。 还有特殊情况，有些测试针对单一节点，在 becomeLeader() 和 Propose() 中要立即尝试更新 committed，否则这个节点将无法更新 committed。 123456789101112131415161718192021222324252627282930func (r *Raft) handleHeartBeatResponse(m pb.Message) &#123; if r.Term &lt; m.Term &#123; if r.State != StateFollower &#123; r.becomeFollower(m.Term, None) return &#125; &#125; r.resHeartBeat[m.From] = true if m.Commit &lt; r.RaftLog.committed &amp;&amp; r.State == StateLeader &#123; // 更新commit r.sendAppend(m.From) &#125;&#125;func (r *Raft) handleAppendResponse(m pb.Message) &#123; if m.Reject &#123; ... &#125; else &#123; if m.Index &gt; r.Prs[m.From].Match &#123; r.Prs[m.From].Match = m.Index &#125; r.Prs[m.From].Next = r.Prs[m.From].Match + 1 if r.updateCommit() &#123; // 更新commit for pr := range r.Prs &#123; if pr != r.id &#123; r.sendAppend(pr) &#125; &#125; &#125; &#125;&#125; 关于 2a 中很多测试细节会让人感到烦躁并思考是否有必要，但这一切会保证在后续的多线程测试中对 raft 少改点。。。 rawnoderawnode 先前提过，上层应用会通过 ready 结构来进行日志应用和持久化等操作。 首先看 ready 结构体： 1234567891011121314type Ready struct &#123; *SoftState // 节点当前的易失状态，当 softState 为 nil 时说明没有更新 pb.HardState // 在消息发送前需要保存的状态，没有更新时是空的 Entries []pb.Entry // 未持久化的条目，即 stable 后的日志条目 Snapshot pb.Snapshot // 需要应用的日志 CommittedEntries []pb.Entry // 已经提交但没有应用到状态机的日志条目 Messages []pb.Message // raft 节点需要转发的消息&#125; 上层应用检查 ready 是否有更新，对于 softState 和 hardState 需要记录旧的状态进行对比，需要在 rawnode 中记录旧状态，而其余部分只要出现就需要更新。 123456789101112131415161718func (rn *RawNode) HasReady() bool &#123; // Your Code Here (2A). hardState := rn.Raft.hardState() softState := rn.Raft.softState() if !IsEmptyHardState(hardState) &amp;&amp; !isHardStateEqual(hardState, rn.oldHard) &#123; return true &#125; if !softState.isSoftStateEqual(rn.oldSoft) || !IsEmptySnap(rn.Raft.RaftLog.pendingSnapshot) || len(rn.Raft.msgs) &gt; 0 || len(rn.Raft.RaftLog.unstableEntries()) &gt; 0 || len(rn.Raft.RaftLog.nextEnts()) &gt; 0 &#123; return true &#125; return false&#125; 将在 project2b 中进一步理解上层是如何通过 ready 来应用新状态的。上层节点对 ready 进行应用和持久化，反过来 rawnode 就要更新状态，将 stabled 和 applied 更新。 123456789101112131415func (rn *RawNode) Advance(rd Ready) &#123; // Your Code Here (2A). if !IsEmptyHardState(rd.HardState) &#123; rn.oldHard = rd.HardState &#125; if len(rd.Entries) &gt; 0 &#123; rn.Raft.RaftLog.stabled = rd.Entries[len(rd.Entries)-1].Index &#125; if len(rd.CommittedEntries) &gt; 0 &#123; rn.Raft.RaftLog.applied = rd.CommittedEntries[len(rd.CommittedEntries)-1].Index &#125; rn.Raft.RaftLog.pendingSnapshot = nil rn.Raft.msgs = nil rn.Raft.RaftLog.maybeCompact()&#125; SoftState 和 HardState 为何要记录这些内容？ 存入 SoftState 的内容反映了当下服务器节点的身份和 leader，仅供上层判断是否有内容更新，当服务点故障重启时先前的状态不应当恢复，RaftState 为 follower，Lead 为 None。 1234type SoftState struct &#123; Lead uint64 RaftState StateType&#125; 存入 hardState 内的状态是需要持久化的，对于 term，重启的节点能明确自己所处的选举轮次，leader 在网络隔离后无法 commit 日志，其余的节点故障重启会投票产生一个较小 term （例如 term 1）的 leader 并 commit term 1 的日志，然后较高 term 的 leader 回归后发生混乱。对于 vote，防止选举期间的混乱，在一个任期内一个节点故障之后恢复，触发了两次投票。对于 commit，记录了当前节点已经提交的日志条目的索引值。持久化 commit 能让节点在重启后知道哪些日志已经被成功提交到状态机，便于进一步同步。由于 raft 日志具有幂等性，仅需记录 commit 日志位置即可。 12345type HardState struct &#123; uint64 term uint64 vote uint64 commit&#125; 似乎写不完 project2b 和 project2c 的总结了，这两才是最复杂的部分，留到下一篇日志吧。。。","categories":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://wizardtot.github.io/categories/TinyKV/"}],"tags":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://wizardtot.github.io/tags/TinyKV/"}],"author":"WizardTOT"},{"title":"raft 总结","slug":"raft总结","date":"2025-01-18T06:39:12.000Z","updated":"2025-05-31T08:15:55.521Z","comments":true,"path":"2025/01/18/raft总结/","permalink":"https://wizardtot.github.io/2025/01/18/raft%E6%80%BB%E7%BB%93/","excerpt":"这是 MIT6.824 课程 raft 论文阅读以及 lab2 总结贴","text":"这是 MIT6.824 课程 raft 论文阅读以及 lab2 总结贴 论文总结论文原文：http://nil.csail.mit.edu/6.824/2022/papers/raft-extended.pdf 本篇论文是 MIT6.824 的第四篇论文，中间的那两篇有时间再补吧 论文背景文中说 raft 是一种相较于 paxos 更易理解的共识算法，但个人感觉论文与后面 lab 实现时还有不少地方需要自己思考或者search 共识算法即让一个集群在可能出现故障情况下完成一项任务 复制状态机提升容错的方法就是冗余，复制状态机就是用于解决容错性的问题。如图所示，复制状态机是一组机器的副本，包含机器状态和一系列log，顺序执行一致的log在没有故障时将得到相同的结果。 raft算法就要保证复制 log 的一致性，这样即便一些副本出现故障，剩余副本的命令被正确复制，每个服务器的状态机就会按日志顺序处理它们，并将输出返回给客户端。 raft共识算法raft 管理复制状态机，在副本中选择一个作为 leader，leader 与客户端交互，获得的 log 条目复制到其他的副本中，数据由 leader 流向客户端，当领导者故障时就会选取新的领导者。 raft基本通常副本有三种状态： leader 处理客户端的请求，向所有状态机发送心跳，复制 entry 到状态机中。 candidate leader 选举时的状态，当 leader 故障时成为 candidate 并收集选票。 follower 被动响应请求，在 leader 超时时转换为 candidate 三种状态有以下的转换关系 时间被划分为任期 Term，每个 Term 开头是选举，没有选出 leader 则 Term 结束。 Term 用连续的整数编号。每个 Term 都以选举开始。Term 在 Raft 中充当逻辑时钟，允许服务器检测过时的信息。为此，每个服务器存储一个 CurrentTerm 变量，并在进行通信时交换该变量以保证一致性。一旦收到的消息中包含更新的 Term，服务器会将 CurrentTerm 更新为较大的那个，并转换为 Follwer。 leader选举触发 leader 选举的条件为当 leader 的 CurrentTerm 并非最大时（这是可能出现的情况），或者 leader 下线。 raft 使用心跳机制来触发选举，leader 向其他 follower 发送心跳维护领导，即发送空的 AppendEntries RPC，这个 RPC 能维护 follower 的当前的状态并重置选举时间计时器，当 follower 在 ElectionTimeOut 内没有收到该 RPC 则转为 candidate 进行选举。 这时在短时间一定有很多 follower 成为 candidate 触发选举，raft 设置随机的 ElectionTimeOut 来解决这个问题。 过了随机 ElectionTimeOut 后 follower 要成为 candidate 并向其他 follower 发送 requestVote RPC，如果 follower 的 term 要高于 candidate 的 term 就不投票，若为投给别的 candidate 就比较 log ，如果 follower log 的 term 更新就不投票，如果 term 相等，有更新的 log 就不投票。 选举时 candidate 会遇到以下情况： 获得大多数 raft 的投票，转换为 leader 发送心跳维护领导。 另一个 raft 当选 leader，会收到 AppendEntries RPC，如果 RPC 中 term &gt;&#x3D; CurrentTerm 就转换为 follower , 否则保持。 出现分裂投票导致选举超时未成功，则重新开始选举，并且每个 raft 都要在随机时间进行。 日志复制在复制状态机里 log 组织方式如下，log 有 index 以及对应的 term, 给定 index 和 term 这个日志中的内容就是确定的，在所有的状态机中不会改变。 为保持 log 复制时的一致性，follower 要从 leader 中复制日志，正常情况下复制不会有什么问题，但当 leader 出现故障时就会出现各种不一致的情况，此时就要约束日志复制的过程。 日志复制的流程如下： leader 对其他的服务器会维护 nextIndex 以及 matchIndex，nextIndex 记录leader 将发送给 follower 的下一个 index，当首次当选 leader 时将所有的 nextIndex 初始化为最后一个 index 即可。 leader 收到客户端中的命令作为 entry 写入新的日志项，向其他的服务器发送 AppendEntries RPC 复制日志，同时捎带 PrevLogIndex 和 PrevLogTerm。 收到 RPC 后 follower 寻找 Index &#x3D;&#x3D; PrevLogIndex &amp;&amp; Term &#x3D;&#x3D; PrevLogTerm 的 entry，没有找到就拒绝，收到回复后 leader 就减少nextIndex 并重新发送 RPC，最终使 nextIndex 到达 leader 和 follower 匹配的位置。匹配方式可以进一步优化，拒绝 AppendEntries RPC 后 follower 可以发送冲突 entry 的 term 以及该 term 中第一个 index，这样 leader 便可以绕过该 term 中所有冲突的 entry。 接受 AppendEntries RPC 后 follower 就需要删除冲突的 entry 并添加上 AppendEntries RPC 中的 entry ，这样便实现 log 的一致性。 复制到大多数（和选举时类似） raft 的 entry 会标记为 committed，所有的 服务器都维护 commitIndex 记录，leader 会将 committed 的 entry 应用并将执行结果返回到客户端。 一旦 Follwer 收到一个更大的 leadercommit，说明有新的 Entry 已提交，它会按序将其 Apply 于其本地。这里如果 leadercommit 如果大于 commitIndex，则 commitIndex 设置为 最后一个日志的 index 以及 leadercommit 中的最小值。 安全性选举约束上述限制还不足以应对 leader 故障后选举的情况，raft 保证 entry 仅从 leader 流向 follower ，而 leader 不会覆盖日志中的现有条目。 因此选举时要增加约束，candidate 日志中最后一个 entry 应当比 follower 的最后一个 entry 更新，否则 follower 就会拒绝投票，这里的更新可以理解为 term 更大或者具有相同的 term 时 index 更大。 提交旧 entry上面提到 entry 被复制到大多数服务器时就会被提交，但一个新 leader 是无法知道以前 term 已经复制到大多数服务器的 entry 是否被提交，下图所示的情况说明这一情形。 对于这个问题，raft 规定只提交当前 term 的 entry，不能提交旧 term 的 entry，这种方式让先前的 entry 被间解提交。 此时上图中 c 情况下 term 2 的 entry 无法被 s1 提交，只有出现 e 情况下才能被间接提交，出现 d 中情况客户端会监测到 2 超时从而重新发送该 entry，即便覆盖也不会出现问题。 follower 和 candidate 故障如果 follower 和 candidate 出现故障，则发送的 RequestVote 和 AppendEntries 将不会产生回复，raft 将无限期尝试处理失败，服务器重启后这些 RPC 将成功完成，如果接收到 RPC 后完成处理但在响应时故障，重启时会收到相同的 RPC ，服务器会忽略这些条目。 集群成员变更更改集群部分成员时，会出现集群分裂的情况，当然在同一个 term 内是不允许出现两个 leader 当选的。此时要做以下的联合共识的过程： entry 被复制两个配置中所有的服务器 任何服务器都能充当 leader 选举和提交的 entry 要来自新旧配置服务器中的大多数 这样的联合共识可以保障切换配置时的安全性。 日志压缩raft 使用快照机制来压缩日志信息 写入快照记录以下元数据： Last Included Index: 快照包含日志中最后一个 entry 的 index Last Included Term: 该 entry 的 term 当 leader 建立快照丢弃了发送给 follower 的下一个 entry 时，需要使用 InstallSnapshot 的 RPC 将快照发送给 follower 当 follower 收到这个 RPC 时： 如果 LastIncludedIndex &gt;&#x3D; follower 最后一个 entry 的 index 时，则 follower 就丢弃日志，并覆盖快照 相反覆盖的 entry 就丢弃，快照后的条目有效就保留 何时写入快照？ 写快照必然会花费大量时间，可以等到日志达到一定大小后就建立快照，还有可以使用 COW 技术，在不影响写入快照时更新日志。 lab总结梳理几个关键的地方，按论文说法就分解成这几个部分 选举在lab中选举的基本流程可以这样概括： 初始化后所有 raft 为 follower，随机一个定时ElectionTime，当时间到达后 raft 进入选举状态，满足以下条件时： 未收到来自 leader 的心跳 没有投票给其他节点 没有发起选举 即可转换为 candidate，并增加 currentTerm，向其他的 raft 发送 requestVote RPC, 其余 raft 在接收到 RPC 满足论文中所说条件时回复 当统计有半数 raft 以上赞成时成为 leader 立刻初始化并开始 log replication tickerlab 中不推荐使用 timer 进行计时，而是使用 time.sleep，这样也十分方便，当election 计时结束未及时收到来自 leader 的 RPC 后即可尝试进行选举 123456789101112131415161718func (rf *Raft) ticker() &#123; for !rf.killed() &#123; // Your code here to check if a leader election should // be started and to randomize sleeping time using // time.Sleep(). time.Sleep(ElectionTime + time.Duration(rand.Int63n(300))*time.Millisecond) // go rf.CheckQuorum() if rf.role == Leader &#123; continue &#125; if time.Since(rf.lastTimeBeat) &gt; ElectionTime &#123; fmt.Printf(&quot;%d want a new leader with term %d\\n&quot;, rf.me, rf.currentTerm) go rf.PreVote() &#125; &#125;&#125; candidate满足条件后开始进行选举，我的做法是建立一个管道，收集 reply，然后统计管道中赞成票数量，只要收到更高 term 的反对票就要放弃 candidate 身份重新设置计时，对于更低 term 的拒绝票显然是过期的回复。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364func (rf *Raft) leaderElection() &#123; rf.mu.Lock() defer rf.mu.Unlock() if rf.role == PreCandidate &amp;&amp; time.Since(rf.lastTimeBeat) &gt; ElectionTime &#123; rf.role = Candidate &#125; if rf.role == Candidate &amp;&amp; time.Since(rf.lastTimeBeat) &gt; ElectionTime &#123; rf.votedFor = rf.me rf.currentTerm++ rf.persist() rf.ResetTime() &#125; args := RequestVoteArgs&#123; Term: rf.currentTerm, CandidateID: rf.me, LastLogIndex: rf.log.LastIndex(), LastLogTerm: rf.log.LastTerm(), &#125; //fmt.Printf(&quot;raft %d self-vote\\n&quot;, rf.me) votedch := make(chan *RequestVoteReply, len(rf.peers)) defer close(votedch) for peer := range rf.peers &#123; if peer == rf.me &#123; continue &#125; go func(peer int) &#123; reply := RequestVoteReply&#123;&#125; if rf.sendRequestVote(peer, &amp;args, &amp;reply) &#123; //fmt.Printf(&quot;raft %d send request to %d\\n&quot;, rf.me, peer) votedch &lt;- &amp;reply &#125; else &#123; votedch &lt;- nil //fmt.Printf(&quot;raft %d failed to send request to %d\\n&quot;, rf.me, peer) &#125; &#125;(peer) &#125; rf.mu.Unlock() countVote := 1 majority := (len(rf.peers))/2 + 1 for i := 0; countVote &lt; majority &amp;&amp; i &lt; len(rf.peers)-1; i++ &#123; reply := &lt;-votedch if reply != nil &#123; if reply.VoteGranted &#123; countVote += 1 //fmt.Printf(&quot;raft %d reply successfully from %d\\n&quot;, rf.me, reply.Peer) &#125; else &#123; rf.mu.Lock() if reply.Term &gt; rf.currentTerm &#123; rf.UpdateTerm(reply.Term) //fmt.Printf(&quot;raft %d reply rejection from %d\\n&quot;, rf.me, reply.Peer) rf.mu.Unlock() break &#125; rf.mu.Unlock() &#125; &#125; &#125; rf.mu.Lock() if countVote &gt;= majority &amp;&amp; rf.role == Candidate &amp;&amp; rf.currentTerm == args.Term &#123; rf.BecomeLeader() &#125;&#125; 成为 leader 后要初始化 nextIndex，立即进行一次心跳 123456789101112func (rf *Raft) BecomeLeader() &#123; rf.leaderID = rf.me rf.role = Leader fmt.Printf(&quot;raft %d become leader with term %d\\n&quot;, rf.me, rf.currentTerm) rf.nextIndex = make([]int, len(rf.peers)) for peer := range rf.peers &#123; rf.nextIndex[peer] = rf.log.LastIndex() + 1 &#125; rf.matchIndex = make([]int, len(rf.peers)) rf.NotifyHeartBeat() go rf.LeaderHeartBeat()&#125; followerfollower 没有投过票才能进行投票，赞成票就两种情况 term比 candidate 低 candidate 具有更新的 log 12345678910111213141516171819202122232425func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) &#123; // Your code here (2A, 2B). rf.mu.Lock() defer rf.mu.Unlock() reply.Term = rf.currentTerm reply.VoteGranted = false reply.Peer = rf.me if args.Term &lt; rf.currentTerm &#123; return &#125; if args.Term &gt; rf.currentTerm &#123; rf.UpdateTerm(args.Term) &#125; // 判断up-to-date,如果rf的term更新就不投票，如果rf的term相等，有更新的log就不投票 if rf.votedFor == -1 || rf.votedFor == args.CandidateID &#123; if args.LastLogTerm &gt; rf.log.LastTerm() || (args.LastLogTerm == rf.log.LastTerm() &amp;&amp; args.LastLogIndex &gt;= rf.log.LastIndex()) &#123; reply.VoteGranted = true rf.votedFor = args.CandidateID rf.persist() rf.ResetTime() &#125; return &#125;&#125; prevote在测试时我发现了这样的问题，被隔离的 raft 重新加入集群时其 term 会非常高，隔离的 raft 不会收到心跳并不断发起选举增加 term，重新加入后会打断正常运行并发起选举，这对 raft 算法而言是小问题，当然可以做如下改进增强稳健性。 这是原论文对这种问题提出的一种优化方法，在正式选举前进行一轮预选举，类似选举但不增加 term，不会显著影响选举的性能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func (rf *Raft) PreVote() &#123; rf.mu.Lock() if rf.role == Leader || time.Since(rf.lastTimeBeat) &lt; ElectionTime &#123; rf.mu.Unlock() return &#125; if rf.role == PreCandidate &#123; rf.mu.Unlock() go rf.leaderElection() return &#125; args := RequestVoteArgs&#123; Term: rf.currentTerm + 1, CandidateID: rf.me, LastLogIndex: rf.log.LastIndex(), LastLogTerm: rf.log.LastTerm(), &#125; rf.mu.Unlock() votedch := make(chan *RequestVoteReply, len(rf.peers)) for peer := range rf.peers &#123; if peer == rf.me &#123; continue &#125; go func(peer int) &#123; reply := RequestVoteReply&#123;&#125; if rf.sendRequestPreVote(peer, &amp;args, &amp;reply) &#123; votedch &lt;- &amp;reply &#125; else &#123; votedch &lt;- nil &#125; &#125;(peer) &#125; countVote := 1 majority := (len(rf.peers))/2 + 1 for i := 0; countVote &lt; majority &amp;&amp; i &lt; len(rf.peers)-1; i++ &#123; reply := &lt;-votedch if reply != nil &#123; if reply.VoteGranted == true &#123; countVote++ //fmt.Printf(&quot;raft %d get prevote from %d\\n&quot;, rf.me, reply.Peer) &#125; &#125; &#125; rf.mu.Lock() defer rf.mu.Unlock() if countVote &gt;= majority &amp;&amp; rf.currentTerm == args.Term-1 &#123; rf.role = PreCandidate go rf.leaderElection() &#125;&#125; 复制leader完成选举后 leader 就要开始进行复制，leader 利用 Start() 接收客户端发送的command，一旦有新 command 过来就要打包成日志并立即发送 appendEntries RPC 同步到 follower 中。除此之外 leader 还要在 HeartBeatTime 进行发送 appendEntries RPC 进行心跳（当没有可发送日志时就是空的） 1234567891011121314151617181920212223242526func (rf *Raft) NotifyHeartBeat() &#123; select &#123; case rf.heartBeatCh &lt;- true: default: &#125;&#125;func (rf *Raft) LeaderHeartBeat() &#123; for !rf.killed() &#123; select &#123; case _, ok := &lt;-rf.heartBeatCh: if !ok &#123; return &#125; case &lt;-time.After(HeartBeatTime): &#125; rf.mu.Lock() if rf.role == Leader &#123; rf.mu.Unlock() rf.LogReplication() continue &#125; rf.mu.Unlock() return &#125;&#125; 在复制日志时如果匹配的日志 index 低于 SnapShotIndex 时需要发送 SnapShot ，发送 AppendEntryArgs 后就要启动一个线程进行后处理 12345678910111213141516171819202122232425262728293031323334func (rf *Raft) LogReplication() &#123; for peer := range rf.peers &#123; if peer == rf.me &#123; continue &#125; rf.mu.Lock() if rf.nextIndex[peer]-1 &lt; rf.SnapShotIndex &#123; args := SnapShotArgs&#123; Term: rf.currentTerm, LeaderId: rf.me, LastIncludedIndex: rf.SnapShotIndex, LastIncludedTerm: rf.SnapShotTerm, Data: rf.SnapShotData, &#125; rf.mu.Unlock() reply := SnapShotReply&#123;&#125; go rf.HandleInstallSnapShot(peer, &amp;args, &amp;reply) &#125; else &#123; prevLogIndex := rf.nextIndex[peer] - 1 args := AppendEntryArgs&#123; Term: rf.currentTerm, LeaderID: rf.me, PrevLogIndex: prevLogIndex, PrevLogTerm: rf.log.GetEntry(prevLogIndex).Term, Entries: rf.log.GetBack(prevLogIndex + 1), LeaderCommit: rf.commitIndex, QuorumCheck: false, &#125; rf.mu.Unlock() reply := AppendEntryReply&#123;&#125; go rf.HandleAppendEntries(peer, &amp;args, &amp;reply) &#125; &#125;&#125; 当日志被复制到大部分服务器时就要 commit，意思是每次成功 append entry 后尝试更新 commitIndex 为半数以上 raft 拥有的日志 index，在 commitIndex 之前的日志都可以认为是已经提交的，更新后就要通知 applier 将这些已提交但未应用的日志应用。 在论文中提到，为保证安全性，所有非当前的 term 的日志要间接提交，即 leader 只能提交当前 term 的日志。 1234567891011121314func (rf *Raft) Commit() &#123; var SortedMatchIndex []int SortedMatchIndex = append(SortedMatchIndex, rf.matchIndex...) SortedMatchIndex[rf.me] = rf.log.LastIndex() sort.Ints(SortedMatchIndex) commitIndex := SortedMatchIndex[(len(SortedMatchIndex)+1)/2-1] if commitIndex &lt;= rf.SnapShotIndex &#123; return &#125; if commitIndex &gt; rf.commitIndex &amp;&amp; (commitIndex == SortedMatchIndex[0] || rf.log.GetEntry(commitIndex).Term == rf.currentTerm) &#123; rf.commitIndex = commitIndex rf.applyCond.Broadcast() &#125;&#125; follower本 lab 中需要考虑最多的部分，follower 要接收心跳、接收日志，需要满足以下规则： 如果 currentTerm &gt; leader’s term，接收失败，否则重置心跳时间重置身份，更新 Term 如果存在冲突，即 prevLogIndex 处的 term 与 PrevLogTerm 不一致，拒绝接收，删除日志中这个条目及之后的内容 如果 prevLogIndex &lt; snapshotIndex，只添加或替换 snapshotIndex 之后的条目 在日志中添加条目时只添加不存在于 log 中的部分 commit 时取 leader commitIndex 与 最后一条日志序号的最小值作为 commitIndex123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172func (rf *Raft) AppendEntries(args *AppendEntryArgs, reply *AppendEntryReply) &#123; rf.mu.Lock() defer rf.mu.Unlock() if args.QuorumCheck &#123; reply.IsSuccess = true rf.ResetTime() return &#125; reply.Term = rf.currentTerm if rf.currentTerm &gt; args.Term &#123; reply.IsSuccess = false fmt.Printf(&quot;raft %d term &gt; leader %d\\n&quot;, rf.me, args.LeaderID) return &#125; rf.ResetTime() rf.leaderID = args.LeaderID if rf.currentTerm &lt; args.Term || rf.role == Candidate || rf.role == PreCandidate &#123; rf.UpdateTerm(args.Term) &#125; // record the conflicted term and index if args.PrevLogIndex &gt; rf.log.LastIndex() &#123; reply.IsSuccess = false reply.ConflictTerm = rf.log.LastTerm() reply.ConflictIndex = rf.log.LastIndex() //fmt.Printf(&quot;raft %d last index %d &gt;= prev index %d\\n&quot;, rf.me, rf.log.LastIndex(), args.PrevLogIndex) return &#125; if args.PrevLogIndex &lt; rf.SnapShotIndex &#123; i := rf.SnapShotIndex - args.PrevLogIndex - 1 if len(args.Entries) &gt; i &#123; rf.log.ReplaceEntry(rf.SnapShotIndex, args.Entries[i:]...) rf.persist() if args.LeaderCommit &gt; rf.commitIndex &#123; if args.LeaderCommit &gt; rf.log.LastIndex() &#123; rf.commitIndex = rf.log.LastIndex() &#125; else &#123; rf.commitIndex = args.LeaderCommit &#125; //fmt.Printf(&quot;Follower %d commit index %d with term %d\\n&quot;, rf.me, rf.commitIndex, rf.log.GetEntry(rf.commitIndex).Term) rf.applyCond.Broadcast() &#125; &#125; reply.IsSuccess = true return &#125; if args.PrevLogTerm != rf.log.GetEntry(args.PrevLogIndex).Term &#123; fmt.Printf(&quot;raft %d entry %d != prev log term %d\\n&quot;, rf.me, rf.log.GetEntry(args.PrevLogIndex).Term, args.PrevLogTerm) reply.IsSuccess = false reply.ConflictTerm = rf.log.GetEntry(args.PrevLogIndex).Term reply.ConflictIndex = args.PrevLogIndex rf.log.CutBack(args.PrevLogIndex) rf.persist() return &#125; rf.log.ReplaceEntry(args.PrevLogIndex+1, args.Entries...) rf.persist() if args.LeaderCommit &gt; rf.commitIndex &#123; if args.LeaderCommit &gt; rf.log.LastIndex() &#123; rf.commitIndex = rf.log.LastIndex() &#125; else &#123; rf.commitIndex = args.LeaderCommit &#125; //fmt.Printf(&quot;Follower %d commit index %d with term %d\\n&quot;, rf.me, rf.commitIndex, rf.log.GetEntry(rf.commitIndex).Term) rf.applyCond.Broadcast() &#125; reply.IsSuccess = true&#125; 日志后处理在 follower 中要记录冲突的 index 以及 Term 当 prevLogIndex &gt; log 中最后的 index，ConflictIndex 即为 log 中最后的 index，ConflictTerm 为对应的 Term 当 prevLogIndex 处的 term 与 PrevLogTerm 不一致，ConflictIndex 为不一致位置的 index，ConflictTerm 为对应的不一致的 Term 在 leader 中正常成功添加日志后，matchIndex 应当更新为 prevLogIndex + len(Entries), 在非正常回复下： 当 follower 的 term 大于 leader 的 term 时 leader 要更新 term 并退化 follower 当发生冲突时，如果 prevLogIndex 在日志中存在(当然首先得 &gt;&#x3D; SnapShotIndex )时，但是 term 与 prevLogTerm 不一致，此时要从 conflictIndex 开始递减寻找 conflictTerm 中最后一个索引条目之外的条目。 其余情况下设置 nextIndex 为 conflictIndex + 112345678910111213141516171819202122232425262728293031323334353637func (rf *Raft) HandleAppendEntries(peer int, args *AppendEntryArgs, reply *AppendEntryReply) &#123; if ok := rf.peers[peer].Call(&quot;Raft.AppendEntries&quot;, args, reply); ok &#123; rf.mu.Lock() defer rf.mu.Unlock() if reply.IsSuccess &#123; //fmt.Printf(&quot;leader %d send reply seccessfully to %d\\n&quot;, rf.me, whichServer) matchIndex := args.PrevLogIndex + len(args.Entries) if matchIndex &gt; rf.matchIndex[peer] &#123; rf.matchIndex[peer] = matchIndex rf.Commit() &#125; rf.nextIndex[peer] = rf.matchIndex[peer] + 1 &#125; else &#123; //fmt.Printf(&quot;leader %d send reply unseccessfully to %d\\n&quot;, rf.me, whichServer) if reply.Term &gt; rf.currentTerm &#123; rf.UpdateTerm(reply.Term) &#125; else &#123; var newIndex int if reply.ConflictIndex &lt; rf.SnapShotIndex &#123; newIndex = reply.ConflictIndex + 1 &#125; else if rf.log.GetEntry(reply.ConflictIndex).Term == reply.ConflictTerm &#123; newIndex = reply.ConflictIndex + 1 &#125; else &#123; term := rf.log.GetEntry(reply.ConflictIndex).Term for newIndex = reply.ConflictIndex; newIndex &gt; rf.log.FirstIndex() &amp;&amp; rf.log.GetEntry(newIndex).Term == term; newIndex-- &#123; &#125; newIndex += 1 &#125; if newIndex &gt; rf.matchIndex[peer]+1 &#123; rf.nextIndex[peer] = newIndex &#125; else &#123; rf.nextIndex[peer] = rf.matchIndex[peer] + 1 &#125; &#125; &#125; &#125;&#125; applier在 lab 中，commitIndex 记录已提交的条目 index，lastApplied 记录已应用的条目 index, 有两种情况要及时应用 log 中的条目 snapshotIndex &gt; rf.lastApplied，此时要将快照信息发送到 applych 管道中 当 commitIndex &gt; rf.lastApplied，此时要将 rf.lastApplied 到 commitIndex 之间所有的条目发送到 applych 管道中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func (rf *Raft) Applier() &#123; for !rf.killed() &#123; rf.mu.Lock() snapshotIndex := rf.SnapShotIndex if snapshotIndex &gt; rf.lastApplied &#123; //fmt.Printf(&quot;------raft %d apply snapshotindex %d last applied %d with commitindex %d-------\\n&quot;, rf.me, rf.SnapShotIndex, rf.lastApplied, rf.commitIndex) newApplyMsg := ApplyMsg&#123; CommandValid: false, SnapshotValid: true, Snapshot: rf.SnapShotData, SnapshotTerm: rf.SnapShotTerm, SnapshotIndex: rf.SnapShotIndex, &#125; // don&#x27;t lock while sending apply msg //fmt.Printf(&quot;raft %d applied snapshot %d snapshot %v\\n&quot;, rf.me, rf.SnapShotIndex, rf.SnapShotData) rf.mu.Unlock() rf.applyCh &lt;- newApplyMsg rf.mu.Lock() if snapshotIndex &gt; rf.lastApplied &#123; rf.lastApplied = snapshotIndex if snapshotIndex &gt; rf.commitIndex &#123; rf.commitIndex = snapshotIndex &#125; &#125; rf.mu.Unlock() &#125; else if rf.commitIndex &gt; rf.lastApplied &#123; //fmt.Printf(&quot;------raft %d apply snapshotindex %d last applied %d with commitindex %d-------\\n&quot;, rf.me, rf.SnapShotIndex, rf.lastApplied, rf.commitIndex) newApplyMsg := ApplyMsg&#123; CommandValid: true, Command: rf.log.GetEntry(rf.lastApplied + 1).Command, CommandIndex: rf.lastApplied + 1, SnapshotValid: false, &#125; //fmt.Printf(&quot;raft %d applied index %d command %v\\n&quot;, rf.me, rf.lastApplied+1, newApplyMsg.Command) rf.mu.Unlock() // don&#x27;t lock while sending apply msg rf.applyCh &lt;- newApplyMsg rf.mu.Lock() if rf.commitIndex &gt; rf.lastApplied &#123; rf.lastApplied = rf.lastApplied + 1 &#125; rf.mu.Unlock() &#125; else &#123; rf.applyCond.Wait() rf.mu.Unlock() &#125; &#125;&#125; checkQuorum这是对复制部分的优化，配合 prevote 一起使用，同样是解决网络分区的问题，在日志中发现当 leader 被隔离时同样在不断发送 appendEntries RPC(这是正常实现时几乎不会出现的问题)，但显然是无效的，基于 appendEntries 在隔一段时间后检查 leader 是否有大部分的 follower 能接收，否则退化为 follower 12345678910111213141516171819202122232425262728293031323334353637383940414243func (rf *Raft) CheckQuorum() &#123; rf.mu.Lock() if rf.role != Leader &#123; rf.mu.Unlock() return &#125; rf.mu.Unlock() QuorumCh := make(chan bool, len(rf.peers)) for peer := range rf.peers &#123; if peer == rf.me &#123; continue &#125; args := AppendEntryArgs&#123; QuorumCheck: true, &#125; go func(peer int) &#123; reply := AppendEntryReply&#123;&#125; if ok := rf.peers[peer].Call(&quot;Raft.AppendEntries&quot;, &amp;args, &amp;reply); ok &#123; if reply.IsSuccess &#123; QuorumCh &lt;- true &#125; else &#123; QuorumCh &lt;- false &#125; &#125; else &#123; QuorumCh &lt;- false &#125; &#125;(peer) &#125; countQuorum := 1 majority := (len(rf.peers))/2 + 1 for i := 0; i &lt; len(rf.peers)-1 &amp;&amp; countQuorum &lt; majority; i++ &#123; check := &lt;-QuorumCh if check &#123; countQuorum++ &#125; &#125; rf.mu.Lock() if countQuorum &lt; majority &amp;&amp; rf.role == Leader &#123; //fmt.Printf(&quot;-------Leader %d want to step back with count Quorum %d------\\n&quot;, rf.me, countQuorum) rf.UpdateTerm(rf.currentTerm) &#125; rf.mu.Unlock()&#125; 快照raft 使用快照机制防止 log 的无限制增长，lab中同样如此，lab 中对于快照实现比较容易，判断是否小于 snapShotIndex , 然后存储 snapshot 并截断日志即可。 12345678910111213func (rf *Raft) Snapshot(index int, snapshot []byte) &#123; rf.mu.Lock() defer rf.mu.Unlock() if rf.SnapShotIndex &gt;= index &#123; return &#125; rf.SnapShotData = snapshot rf.SnapShotIndex = index rf.SnapShotTerm = rf.log.GetEntry(index).Term //fmt.Printf(&quot;raft %d install snapshot index %d\\n&quot;, rf.me, rf.SnapShotIndex) rf.log.CutFront(index) rf.persistSnapshot()&#125; leader 在进行日志复制检查，当有 peer 的匹配 index 小于 snapshotIndex 时 leader 需要发送 InstallSnapshot RPC 来同步快照 收到 InstallSnapshot RPC 后 follower 要先进行检查(term 等等)，然后截断日志并持久化保存，相应的 leader 要更新 nextIndex 和 matchIndex。 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (rf *Raft) InstallSnapshot(args *SnapShotArgs, reply *SnapShotReply) &#123; rf.mu.Lock() defer rf.mu.Unlock() reply.Term = rf.currentTerm if args.Term &lt; rf.currentTerm &#123; return &#125; rf.ResetTime() if args.Term &gt; rf.currentTerm || rf.role == Candidate || rf.role == PreCandidate &#123; rf.UpdateTerm(args.Term) &#125; rf.leaderID = args.LeaderId if rf.SnapShotIndex &gt;= args.LastIncludedIndex || rf.lastApplied &gt;= args.LastIncludedIndex &#123; return &#125; if rf.log.LastIndex() &gt; args.LastIncludedIndex &#123; rf.log.CutFront(args.LastIncludedIndex) &#125; else &#123; rf.log = Log&#123;[]Entry&#123;&#123;Term: args.LastIncludedTerm&#125;&#125;, args.LastIncludedIndex&#125; &#125; rf.SnapShotTerm = args.LastIncludedTerm rf.SnapShotIndex = args.LastIncludedIndex rf.SnapShotData = args.Data //fmt.Printf(&quot;raft %d install snapshot index %d\\n&quot;, rf.me, rf.SnapShotIndex) rf.persistSnapshot() rf.applyCond.Broadcast()&#125;func (rf *Raft) HandleInstallSnapShot(peer int, args *SnapShotArgs, reply *SnapShotReply) &#123; if ok := rf.peers[peer].Call(&quot;Raft.InstallSnapshot&quot;, args, reply); ok &#123; rf.mu.Lock() defer rf.mu.Unlock() if args.Term != rf.currentTerm &#123; return &#125; if reply.Term &gt; rf.currentTerm &#123; rf.UpdateTerm(reply.Term) &#125; else if rf.matchIndex[peer] &lt; args.LastIncludedIndex &#123; rf.matchIndex[peer] = args.LastIncludedIndex rf.nextIndex[peer] = args.LastIncludedIndex + 1 &#125; &#125;&#125; 第一次接触分布式的 debug ，可以看到打了很多很多日志，大部分情况下要记得锁的使用，死锁还好，活锁记得多看看论文，可能是部分约束条件没有添加。","categories":[{"name":"MIT6.824","slug":"MIT6-824","permalink":"https://wizardtot.github.io/categories/MIT6-824/"}],"tags":[{"name":"MIT6.824","slug":"MIT6-824","permalink":"https://wizardtot.github.io/tags/MIT6-824/"}],"author":"WizardTOT"},{"title":"西电操作系统实验攻略","slug":"西电操作系统实验攻略","date":"2025-01-18T05:08:11.000Z","updated":"2025-01-18T05:54:55.077Z","comments":true,"path":"2025/01/18/西电操作系统实验攻略/","permalink":"https://wizardtot.github.io/2025/01/18/%E8%A5%BF%E7%94%B5%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E9%AA%8C%E6%94%BB%E7%95%A5/","excerpt":"西电计科大二操作系统实验（非软工李航OS课设）","text":"西电计科大二操作系统实验（非软工李航OS课设） 计科的 OS 实验很水，意思意思前几个就行，用不着全部做完 实验报告实验题目一实验内容实验 1 创建进程 学会通过基本的 Windows 或者 Linux 进程控制函数，由父进程创建子进程，并实现父子进程协同工作。创建两个进程，让子进程读取一个文件，父进程等待子进程读取 完文件后继续执行，实现进程协同工作。 进程协同工作就是协调好两个进程，使之安排好先后次序并以此 执行，可以用等待函数来实现这一点。当需要等待子进程运行结束 时，可在父进程中调用等待函数。 代码本实验主要使用fork()系统调用，fork()一次调用，两次返回。当程序调用fork()时，它创建了一个新的子进程，这个子进程是当前调用fork()的父进程的副本。在子进程中，fork()返回0，而在父进程中，它返回新创建的子进程的进程PID。如果fork()调用失败，通常由于系统中没有足够的资源来创建新进程，它会返回一个负值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;int main() &#123; pid_t pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork&quot;); exit(pid); &#125; else if (pid == 0) &#123;//子进程 FILE *fp = fopen(&quot;./data.txt&quot;,&quot;w+&quot;); printf(&quot;Child process started: PID = %d\\n&quot;, getpid()); if(fp)&#123; const char *s1 = &quot;春江潮水连海平\\n&quot;; const char *s2 = &quot;海上明月共潮生\\n&quot;; fwrite(s1,strlen(s1),1,fp); fwrite(s2,strlen(s2),1,fp); &#125;else&#123; printf(&quot;child process failed to write data.\\n&quot;); &#125; fclose(fp); char content[200]; fp = fopen(&quot;./data.txt&quot;,&quot;r&quot;); fread(content, sizeof(char), 100, fp); std::cout &lt;&lt; &quot;child process writed data:\\n&quot; &lt;&lt; content; fclose(fp); &#125; else &#123;//父进程 printf(&quot;Parent process started: PID = %d\\n&quot;, getpid()); int sum = 0; for (int i = 1; i &lt;= 100; i++) &#123; sum = sum + i; printf(&quot;%d\\n&quot;,sum); &#125; std::cout &lt;&lt; &quot;Wait for child process...\\n&quot;; int status; waitpid(pid, &amp;status, 0);//等待子进程运行结束 std::cout &lt;&lt; &quot;Child process exited.\\n&quot;; FILE *fp = fopen(&quot;./data.txt&quot;, &quot;r&quot;); char content[200]; fread(content, sizeof(char), 100, fp); fclose(fp); std::cout &lt;&lt; &quot;Parent process read data:\\n&quot; &lt;&lt; content; std::cout &lt;&lt; &quot;Parent process exited.\\n&quot;; &#125; return 0;&#125; 运行结果图一是实验运行结果，图二是子进程写入文件中的内容，可以看到父进程成功实验fork()创建子进程，先打印运算1至50的累计和，然后等待子进程运行结束，子进程运行时向文件中写入“春江潮水连海平，海上明月共潮生”随后结束运行，父进程从文件中读取子进程写入的数据并打印，程序运行结束。 至于为什么没图片，你自己运行后截图吧.jpg 实验题目二实验内容实验 2 线程共享进程数据了解线程与进程之间的数据共享关系。创建一个线程，在线程中更改进程中的数据。在进程中定义全局共享数据，在线程中直接引用该数据进行更改并输出该数据。 代码本实验主要使用了POSIX线程库中的两个重要函数来创建和同步线程，pthread_create()和pthread_join()。pthread_create()函数原型如下： 12int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg); phtread_create()用于创建一个新的线程，thread: 指向 pthread_t 类型变量的指针，该变量用于唯一标识新创建的线程。attr: 指向线程属性对象的指针。可以指定线程属性，如堆栈大小、调度策略等。start_routine: 新线程将执行的函数的指针，arg: 传递给 start_routine 函数的参数。在本实验中，创建threadProc()函数传入pthread_create()用于新线程执行，当线程创建成功时，pthread_create ()返回 0，如果失败，则返回错误码。pthread_join()用于等待指定的线程终止，函数原型如下： 1int pthread_join(pthread_t thread, void **retval); thread: 要等待的线程的标识符，该线程应该是由 pthread_create ()创建的。retval: 如果不是 NULL，则指向的位置会被设置为被等待线程的返回值。当成功等待线程终止时，pthread_join 返回 0,如果失败，则返回错误码。 123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;pthread.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;iostream&gt;using namespace std;static int count;void *threadProc(void *arg) &#123; cout &lt;&lt; &quot;new thread started.\\n&quot;; for (count = 1; count &lt;= 10; count += 2) &#123; cout &lt;&lt; &quot;now: count =&quot; &lt;&lt; count &lt;&lt; &quot;\\n&quot;; &#125; cout &lt;&lt; &quot;wait for 3 seconds...\\n&quot;; sleep(3); return 0;&#125;int main() &#123; count = 20; std::cout &lt;&lt;&quot;Main process started!\\n&quot; &lt;&lt; &quot;count = &quot; &lt;&lt; count &lt;&lt; endl; pthread_t tid; unsigned int t = pthread_create(&amp;tid, NULL, threadProc, NULL); if (t) &#123; perror(&quot;pthread_create&quot;); exit(t); &#125; void *retraval = NULL; if (pthread_join(tid, &amp;retraval) != 0) &#123; perror(&quot;thread&quot;); &#125; std::cout &lt;&lt; &quot;Thread ended.\\n&quot;; std::cout &lt;&lt; &quot;Main process ended.\\n&quot; &lt;&lt; &quot;count = &quot; &lt;&lt; count &lt;&lt; endl; return 0;&#125; 运行结果在实验结果中可以看到新线程成功创建并运行，并且静态全局变量count是并发访问的，在线程运行时，count值改变并影响到主进程，最后结束时count值为11而非初始设定的20。 实验题目三实验内容利用信号通信机制在父子进程及兄弟进程间进行通信。父进程创建一个有名事件，由子进程发送事件信号，父进程获取事件信号后进行相应的处理。 代码本实验主要使用POSIX有名信号量在父子进程之间实现通信，本实验通过fork()创建子进程，父进程注册了 SIGUSR1 信号的处理函数 handle_sigusr1，在子进程中会提示用户是否要向父进程发送 SIGUSR1 信号，当接收到这个信号时，会调用这个函数。 父进程在创建信号量后会调用 sem_timedwait（），在指定的时间内等待信号量的值增加。如果在 10 秒内信号量的值被增加（即子进程发送了信号并且信号处理函数被调用），父进程会继续执行并打印一条消息表示信号量已经被处理。如果 10 秒内没有收到信号，sem_timedwait 会因超时而返回，并打印一条超时消息。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;signal.h&gt;#include &lt;fcntl.h&gt;#include &lt;sys/stat.h&gt;#include &lt;semaphore.h&gt;#include &lt;iostream&gt;#define SEM_NAME &quot;/my_named_event&quot;// 信号处理函数void handle_sigusr1(int sig) &#123; sem_t *sem = sem_open(SEM_NAME, 0); if (sem != SEM_FAILED) &#123; printf(&quot;Received SIGUSR1, handling the named event...\\n&quot;); sem_post(sem); // 触发事件 sem_close(sem); &#125;&#125;int main() &#123; pid_t pid; // 创建子进程 pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork&quot;); exit(-1); &#125; if (pid == 0) &#123; // 子进程 sleep(3); printf(&quot;Child process started.\\n&quot;); char ch; std::cout &lt;&lt; &quot;Signal the event to Parent[y/n]\\n&quot;; std::cin &gt;&gt; ch; if (ch == &#x27;y&#x27;) &#123; kill(getppid(), SIGUSR1); // 向父进程发送SIGUSR1信号 &#125; exit(0); &#125; else&#123; // 父进程 std::cout &lt;&lt; &quot;Parent process started.\\n&quot;; // 创建有名事件 sem_t *sem; sem = sem_open(SEM_NAME, O_CREAT, 0644, 0); if (sem == SEM_FAILED) &#123; perror(&quot;sem_open&quot;); exit(EXIT_FAILURE); &#125; signal(SIGUSR1, handle_sigusr1); // 设置定时等待事件 struct timespec ts; if (clock_gettime(CLOCK_REALTIME, &amp;ts) == -1) &#123; perror(&quot;clock_gettime&quot;); exit(EXIT_FAILURE); &#125; ts.tv_sec += 10; int ret; while ((ret = sem_timedwait(sem, &amp;ts)) == -1 &amp;&amp; errno == EINTR) &#123; continue; &#125; if (ret == -1) &#123; if (errno == ETIMEDOUT) &#123; printf(&quot;timed out.\\n&quot;); &#125; exit(-1); &#125;else&#123; std::cout &lt;&lt; &quot;Named event handled in parent process.\\n&quot;; &#125; sem_close(sem); sem_unlink(SEM_NAME); &#125; return 0;&#125; 运行结果第一次运行时，在子进程中没有发送SIGUSR1 信号，父进程等待超时结束运行，在第二次运行时，子进程发送SIGUSR1 ，信号处理函数被调用，父进程收到信号打印Named event handled in parent process.。 实验题目四实验内容实验题目 匿名管道通信 学习使用匿名管道在两个进程间建立通信。 分别建立名为 Parent 的单文档应用程序和 Child 的单文档应用程 序作为父子进程，由父进程创建一个匿名管道，实现父子进程向匿 名管道写入和读取数据。 代码本次实验主要使用fork()创建子进程，使用 pipe() 函数创建匿名管道。这个函数需要一个包含两个整数的数组作为参数，成功调用后，这个数组的两个元素会被设置为管道的读端和写端的文件描述符。例如创建fd[2]数组，使用pipe()创建管道后，fd[0] 用于读取管道，fd[1] 用于写入管道。 管道是单向的，数据只能沿一个方向流动，对于父子进程之间双向通信，就必须创建两个管道。close()用于关闭管道，write()用于向管道中写入数据，read()用于向管道中读数据。在向管道写入数据时，要使用close(fd[0])关闭管道读端，在向管道中读取数据时，要使用close(fd[1])关闭管道写端。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;const int BUF_SIZE = 100;int main()&#123; int fd1[2]; if (pipe(fd1) == -1 )//创建子进程读，父进程写管道 &#123; perror(&quot;Fail to create pipe.\\n&quot;); exit(-1); &#125; int fd2[2]; if(pipe(fd2) == -1)&#123;//创建父进程读，子进程写管道 perror(&quot;Fail to create pipe.\\n&quot;); exit(-1); &#125; pid_t pid = fork();//创建进程 if(pid&lt;0)&#123; perror(&quot;Fail to fork.&quot;); exit(pid); &#125; else if(pid ==0)&#123;//子进程 FILE *fp; fp = fopen(&quot;child4.txt&quot;,&quot;w&quot;); char str[] = &quot;--------Child process started.--------\\n&quot;; fwrite(str,strlen(str),1,fp); close(fd1[1]); char message_receive2[BUF_SIZE]; read(fd1[0], message_receive2, BUF_SIZE);//子进程读数据 close(fd1[0]); strcpy(str,&quot;Child process read data from pipe:&quot;); fwrite(str,strlen(str),1,fp); fwrite(message_receive2,strlen(message_receive2),1,fp); char message_to_send2[BUF_SIZE] = &quot;Hello Manchester!\\n&quot;; close(fd2[0]); write(fd2[1],message_to_send2,sizeof(message_to_send2));//子进程写数据 close(fd2[1]); strcpy(str,&quot;Child process writed data in pipe:&quot;); fwrite(str,strlen(str),1,fp); fwrite(message_to_send2,strlen(message_to_send2),1,fp); strcpy(str,&quot;--------Child process exited.--------\\n&quot;); fwrite(str,strlen(str),1,fp); fclose(fp); &#125; else&#123;//父进程 printf(&quot;***Parent process started.***\\n&quot;); close(fd1[0]); char message_to_send1[BUF_SIZE] = &quot;Hala Madrid!\\n&quot;; write(fd1[1], message_to_send1,sizeof(message_to_send1));//父进程写数据 printf(&quot;Parent process writed data in pipe: %s&quot;, message_to_send1); close(fd1[1]); char message_to_receive1[BUF_SIZE]; close(fd2[1]); read(fd2[0],message_to_receive1,BUF_SIZE);//父进程读数据 printf(&quot;Parent process read data from pipe: %s&quot;,message_to_receive1); close(fd2[0]); printf(&quot;***Parent process exited.***\\n&quot;); &#125; exit(0);&#125; 运行结果实验结果可以看出，匿名管道成功创建，实现父子进程之间的通信，父进程向管道中写入Hala Madrid!并被子进程成功读取，子进程向管道中写入Hello Manchester!并被父进程成功读取。 实验题目五实验内容实验 5 命名管道通信 学习使用命名管道在多进程间建立通信。建立父子进程，由父进程创建一个命名管道，由子进程向命名管道写入数据，由父进程从命名管道读取数据。 代码在本实验中，使用fork()创建子进程，使用mkfifo() 创建有名管道，mkfifo() 是一个在 UNIX系统上用于创建有名管道（FIFO）的系统调用。有名管道是一种特殊类型的文件，它允许两个或多个进程实现进程间通信，一旦创建了 FIFO，进程就可以像普通文件一样打开它进行读写操作。一个进程可以写入数据到管道的一端，而另一个进程则可以从管道的另一端读取数据。 FIFO 在文件系统中持久存在，它不像匿名管道那样在进程终止时自动消失，在使用完后要使用unlink()显式删除。 父进程功能：进入循环，先使用open(FIFO_PATH, O_RDONLY)以只读打开 FIFO 以进行读取。调用read(fd, message, BUF_SIZE)从 FIFO 中读取数据到message中，读取后使用close()关闭管道，如果读取到的数据是 “end”，则退出循环，如果读取到的数据不是 “end”，则使用open(FIFO_PATH, O_WRONLY)只写方式打开FIFO，提示用户输入要发送给子进程的数据，并使用write()此数据写入 FIFO。 子进程功能：进入循环，先使用open(FIFO_PATH, O_WRONLY)以只写方式打开 FIFO,提示用户输入要发送给父进程的数据, 使用write()此数据写入 FIFO。写入后使用close()关闭管道，如果写入的数据是 “end”，则退出循环。再使用使用open(FIFO_PATH, O_RDONLY)以只读打开 FIFO，用read()读取父进程发送的数据并打印。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#include &lt;sys/types.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/stat.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#include &lt;iostream&gt;using namespace std;#define FIFO_PATH &quot;./lab5_FIFO&quot;const int BUF_SIZE = 100;int main()&#123; if (mkfifo(FIFO_PATH, 0600) == -1) &#123;//创建有名管道 perror(&quot;mkfifo&quot;); exit(-1); &#125; pid_t pid = fork(); if(pid&lt;0)&#123; perror(&quot;fork&quot;); exit(pid); &#125;else if(pid &gt; 0)&#123;//父进程 printf(&quot;Parent process started: PID = %u\\n&quot;, getpid()); while (1) &#123; int fd = open(FIFO_PATH, O_RDONLY); char message[BUF_SIZE]; if(read(fd, message, BUF_SIZE)&gt;0)&#123;//父进程从管道中读 cout &lt;&lt;&quot;Parent process read from fifo:&quot; &lt;&lt; message &lt;&lt; endl; &#125;else&#123; cout &lt;&lt; &quot;Parent process failed to read.\\n&quot;; exit(-1); &#125; close(fd); if(strcmp(message,&quot;end&quot;)==0)&#123; unlink(FIFO_PATH);//拆除有名管道 cout &lt;&lt; &quot;FIFO is deleted.\\n&quot;; cout &lt;&lt; &quot;Parent process exited.\\n&quot;; exit(0); &#125; fd = open(FIFO_PATH, O_WRONLY); cout &lt;&lt; &quot;Please enter data sent to child process:&quot;; cin &gt;&gt; message; if(write(fd,message,strlen(message)+1)&gt;0)&#123;//父进程从管道中写 cout &lt;&lt; &quot;Parent process successed to write data.\\n&quot;; &#125;else&#123; cout &lt;&lt; &quot;Parent process failed to write data.\\n&quot;; exit(-1); &#125; close(fd); &#125; &#125; else&#123;//子进程 printf(&quot;Child process started. PID = %u\\n&quot;, getpid()); while (1) &#123; int fd = open(FIFO_PATH, O_WRONLY); char message2[BUF_SIZE]; cout &lt;&lt; &quot;Please enter data sent to parent process:&quot;; cin &gt;&gt; message2; if(write(fd, message2, strlen(message2) + 1)&gt;0)&#123;//子进程向管道中写 cout &lt;&lt; &quot;Child process successed to write data.\\n&quot;; &#125;else&#123; cout &lt;&lt; &quot;Child process failed to write data.\\n&quot;; exit(-1); &#125; close(fd); if(strcmp(message2,&quot;end&quot;)==0)&#123; break; &#125; fd = open(FIFO_PATH, O_RDONLY); if(read(fd, message2, BUF_SIZE)&gt;0)&#123;//子进程从管道中读 cout &lt;&lt;&quot;Child process read from fifo:&quot; &lt;&lt; message2&lt;&lt; endl; &#125;else&#123; cout &lt;&lt; &quot;Child process failed to read.\\n&quot;; exit(-1); &#125; close(fd); &#125; printf(&quot;Child process exited.\\n&quot;); &#125;&#125; 运行结果从实验结果可以看到，FIFO成功创建，子进程写入向FIFO中child，父进程成功读取，父进程写入parent，子进程成功读取，子进程写入end时，父子进程结束运行，FIFO删除。 实验题目六实验题目 信号量实现进程同步 进程同步是操作系统多进程&#x2F;多线程并发执行的关键之一，进程 同步是并发进程为了完成共同任务采用某个条件来协调他们的活 动，这是进程之间发生的一种直接制约关系。本次试验是利用信号 量进行进程同步。 生产者进程生产产品，消费者进程消费产品。 当生产者进程生产产品时，如果没有空缓冲区可用，那么生产 者进程必须等待消费者进程释放出一个缓冲区。 当消费者进程消费产品时，如果缓冲区中没有产品，那么消费 者进程将被阻塞，直到新的产品被生产出来。 实验内容本实验使用POSIX 线程库中的pthread_create()创建新线程，使用POSIX信号量实现线程间的同步和互斥模拟生产者和消费者问题，实验初始化了三个信号量 —— mutex 用于互斥访问缓冲区，保证每次只有一个线程可以操作缓冲区；empty 表示缓冲区中空闲位置的数量，初始值为缓冲区大小（BUF_SIZE）；full 表示缓冲区中已填充数据的位置数量，初始值为 0，使用sem_init()对信号量进行初始化，sem_wait()对应P操作，sem_post()对应V操作，信号量使用完后要使用sem_destory()对信号量进行销毁。 生产者线程每2秒生产一个产品，然后尝试将其放入缓冲区。它首先需要等待 empty 信号量（确保缓冲区有空位），然后获取 mutex 信号量（确保互斥访问缓冲区），将产品放入缓冲区，然后释放 mutex 并增加 full 信号量。 消费者线程A和B分别在启动后等待 5 秒和 10 秒，然后开始消费缓冲区中的产品。首先等待 full 信号量（确保缓冲区中有数据），然后获取 mutex 信号量（确保互斥访问缓冲区），从缓冲区取出一个产品，然后释放 mutex 并增加 empty 信号量。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;semaphore.h&gt;#include &lt;pthread.h&gt;#include &lt;stdbool.h&gt;const int BUF_SIZE = 10;int buf[BUF_SIZE];int buf_pointer;sem_t mutex,empty,full;void *Producer(void *arg)&#123; //生产者 int product = 0; printf(&quot;Producer started.\\n&quot;); while(true)&#123; sleep(2); sem_wait(&amp;empty); sem_wait(&amp;mutex); printf(&quot;Producer gets mutex.\\n&quot;); buf[buf_pointer++] = product; printf(&quot;Producer produced: %d\\n&quot;,product); product ++; printf(&quot;Producer releases mutex.\\n&quot;); sem_post(&amp;mutex); sem_post(&amp;full); &#125;&#125;void *Consumer_A(void *arg)&#123;//消费者A sleep(5); printf(&quot;Consumer_A started.\\n&quot;); while(true)&#123; sem_wait(&amp;full); sem_wait(&amp;mutex); printf(&quot;Consumer_A gets the mutex.\\n&quot;); int product_ = buf[--buf_pointer]; printf(&quot;Consumer_A consumed: %d\\n&quot;,product_); printf(&quot;Consumer_A releases mutex.\\n&quot;); sem_post(&amp;mutex); sem_post(&amp;empty); sleep(1); &#125;&#125;void *Consumer_B(void *arg)&#123;//消费者B sleep(10); printf(&quot;Consumer_B started.\\n&quot;); while(true)&#123; sem_wait(&amp;full); sem_wait(&amp;mutex); printf(&quot;Consumer_B gets the mutex.\\n&quot;); int product_ = buf[--buf_pointer]; printf(&quot;Consumer_B consumed: %d\\n&quot;,product_); printf(&quot;Consumer_B releases mutex.\\n&quot;); sem_post(&amp;mutex); sem_post(&amp;empty); sleep(2); &#125;&#125;int main()&#123; sem_init(&amp;mutex, 0, 1);//初始化信号量 sem_init(&amp;empty, 0, 10); sem_init(&amp;full, 0, 0); pthread_t producer_thread,consumerA_thread,consumerB_thread;//创建线程 pthread_create(&amp;producer_thread, NULL, Producer, NULL); pthread_create(&amp;consumerA_thread, NULL, Consumer_A, NULL); pthread_create(&amp;consumerB_thread, NULL, Consumer_B, NULL); int flag = 1;//终止运行 while (flag) &#123; if(getchar())&#123; flag = 0; &#125; &#125; sem_destroy(&amp;mutex); sem_destroy(&amp;empty); sem_destroy(&amp;full); exit(0);&#125; 运行结果从实验结果可以看出，生产者和消费者线程成功创建并运行，互斥进入临界缓存，生产者生产产品后消费者进行消费，生产者也与消费者之间实现同步，键入-1时程序结束运行。 实验题目七实验内容实验题目 共享主存实现进程通信 利用共享主存解决读写者问题。要求写者进程创建一个共享主 存，并向其中写入数据，读者进程随后从该共享主存区中访问数据。 为基于共享主存解决读者-写者问题，需要由写进程首先创建一 个共享主存，并将该共享主存区映射到虚拟地址空间，随后读进程打开共享主存，并将该共享主存区映射到自己的虚拟地址空间，从 中获取数据，并进行处理，以此实现进程通信。 实验代码本次实验使用fork()创建子进程运行，父进程是写者进程，子进程是读者进程，使用POSIX有名信号量实现进程间同步，使用 sem_open() 创建两个信号量 db 和 writeMutex。db 用于控制对共享内存的访问，而 writeMutex 用于保护写者的优先权, 为方便运行，本次实验将信号量通信机制封装在start_write(),end_write(),start_read(),end_read()函数中。 对于创建共享内存，通过 ftok()生成一个键值，然后使用 shmget ()获取共享内存标识符，创建一个新的共享内存段或访问一个已存在的共享内存段， shmat ()将共享内存映射到进程的地址空间, shmdt()将共享内存段从进程的地址空间分离。 程序执行时，父进程（写者）和子进程（读者）都会启动。父进程将从用户那里读取整数并将它们写入共享内存，子进程将等待并读取这些整数。由于使用了 writeMutex 信号量，写操作将始终优先于读操作。这意味着如果写者和读者同时尝试访问共享内存，写者将获得优先权。只有在写者完成写操作并释放了信号量后，读者才能开始读操作。这可以防止读者在写者完成写入之前读取不完整或旧的数据。父进程在写入 -1 到共享内存后结束，然后等待子进程结束。子进程在读取到 -1 后结束。之后，父进程清理了创建的信号量和共享内存段。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125#include &lt;sys/shm.h&gt;#include &lt;sys/wait.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#include &lt;iostream&gt;#include &lt;semaphore.h&gt;#define SHM_SIZE 1024using namespace std;sem_t *db; // 控制对共享内存的访问sem_t *writeMutex; // 保护写者的优先权void start_write() &#123; sem_wait(writeMutex); // 确保写者优先 sem_wait(db); &#125;void end_write() &#123; sem_post(db); sem_post(writeMutex); // 允许读者或写者进入&#125;void start_read() &#123; sem_wait(writeMutex); // 确保写者优先 sem_wait(db); &#125;void end_read() &#123; sem_post(db); sem_post(writeMutex); &#125;int get_shmid()&#123; int key = ftok(&quot;./&quot;, 9); //生成键值 if(key == -1) &#123; perror(&quot;ftok&quot;); &#125; int shm_id = shmget(key, SHM_SIZE, IPC_CREAT|0644);//获取共享内存标识符 if(shm_id &lt; 0) &#123; perror(&quot;shmget&quot;); exit(-1); &#125; return shm_id; &#125;int *create_shm()&#123; int shm_id = get_shmid(); int *shm_add = (int *)shmat(shm_id, NULL, 0);//映射共享内存段 if(shm_add &lt; (int *)0) &#123; perror(&quot;shmat&quot;); exit(-1); &#125; return shm_add;&#125;void del_shm()&#123; int shm_id = get_shmid(); shmctl(shm_id, IPC_RMID, NULL);&#125;int main() &#123; // 初始化信号量 sem_unlink(&quot;db_&quot;); sem_unlink(&quot;writeMutex_&quot;); db = sem_open(&quot;db_&quot;, O_CREAT, 0644, 1); writeMutex = sem_open(&quot;writeMutex_&quot;, O_CREAT, 0644, 1); pid_t pid = fork(); if (pid &lt; 0) &#123; perror(&quot;fork&quot;); exit(-1); &#125; else if (pid &gt; 0) &#123; // 父进程 - 写者 cout &lt;&lt; &quot;Writer process started.\\n&quot;; int *shm_add = create_shm(); int a = 1; while (1) &#123; start_write(); cout &lt;&lt; &quot;Writer process writed:\\n&quot;; int t; cin &gt;&gt; t; shm_add[0] = a; shm_add[a++] = t; end_write(); if(t == -1)&#123; shmdt(shm_add); break; &#125; sleep(3); &#125; wait(NULL); // 等待子进程结束 // 清理信号量 sem_close(db); sem_close(writeMutex); sem_unlink(&quot;db_&quot;); sem_unlink(&quot;writeMutex_&quot;); del_shm(); cout &lt;&lt; &quot;Writer process ended.\\n&quot;; &#125; else &#123;//子进程—读者进程 cout &lt;&lt; &quot;Reader process started.\\n&quot;; int *shm_add = create_shm(); while (1) &#123; start_read(); cout &lt;&lt; &quot;Reader process read:&quot;; for(int i =1;i&lt;=shm_add[0];i++)&#123; cout &lt;&lt; shm_add[i] &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; if(shm_add[shm_add[0]] == -1)&#123; break; &#125; end_read(); sleep(1); &#125; // 清理信号量 sem_close(db); sem_close(writeMutex); cout &lt;&lt; &quot;Reader process ended.\\n&quot;; &#125; return 0;&#125; 运行结果从实验结果中可以看出读者写者进程成功通过共享内存实现进程间通信，写者写入的数据能正确被读者读出，通过有名信号量实现了读者写者对共享内存的同步，并在写者写入-1时，读者写者进程运行结束。 实验题目九实验题目： 通过内核模式显示进程控制块信息 了解 Linux 内核模块变成，熟悉 Linux 内核机制。在内核中，所有的进程控制块都被一个双向链表连接起来， 该链表中的第一个进程控制块为 init_task。编写一个内核模块，模 块接收用户传递的一个参数 num， num 指定要打印的进程控制块的 数量；若用户不指定 num 或者 num&lt;0，模块则打印所有进程控制块的信息，需要打印的进程控制块信息由：进程 PID 和进程的可执行文件名组成。 实验内容实验首先定义静态全局变量 num，默认值为 -1，未指定num值时默认打印所有进程的信息。module_param(num, int, 0644)允许在加载模块时指定一个值，0644 表示权限位（用户可读写，组和其他用户可读）。 随后是模块初始化函数 list_tasks_init(), 这是模块加载时调用的初始化函数。它使用 for_each_process(task) 宏迭代系统中的所有进程，并打印它们的 PID 和命令名（task-&gt;comm）。如果用户指定了 num 参数且不为 -1，则只打印指定数量的进程信息。如果 num 为 -1 或未指定，打印所有进程的信息。 list_tasks_exit()是模块卸载时调用的清理函数。简单地打印“Unloaded module”到内核日志。 MODULE_LICENSE(“GPL”); 指定了模块的许可证，这里是 GNU General Public License。MODULE_AUTHOR 和 MODULE_DESCRIPTION 宏分别提供了模块的作者信息和描述。 编写makefile，生成后使用insmod来加载lab.ko内核模块，使用dmesg即可查看进程的pid和命令名。 实验代码12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/sched/signal.h&gt;#include &lt;linux/moduleparam.h&gt;static int num = -1;module_param(num, int, 0644);MODULE_PARM_DESC(num, &quot;Number of process control blocks to print&quot;);static int __init list_tasks_init(void)&#123; struct task_struct *task; int count = 0; for_each_process(task) &#123; if (num &gt;= 0 &amp;&amp; count &gt;= num) break; printk(KERN_INFO &quot;PID: %d | Command: %s\\n&quot;, task-&gt;pid, task-&gt;comm); count++; &#125; if (num &lt; 0) printk(KERN_INFO &quot;Printed all process control blocks\\n&quot;); else printk(KERN_INFO &quot;Printed %d process control blocks\\n&quot;, count); return 0;&#125;static void __exit list_tasks_exit(void)&#123; printk(KERN_INFO &quot;Unloaded module\\n&quot;);&#125;module_init(list_tasks_init);module_exit(list_tasks_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;Wizard&quot;);MODULE_DESCRIPTION(&quot;A module to list process control blocks&quot;); 运行结果在实验结果中可以看到内核模块顺利载入，未指定num值执行dmesg后打印所有进程的pid和命令名，载入内核模块时指定num&#x3D;5后执行dmesg后打印5个进程的pid和命令名。","categories":[{"name":"实验","slug":"实验","permalink":"https://wizardtot.github.io/categories/%E5%AE%9E%E9%AA%8C/"}],"tags":[{"name":"实验","slug":"实验","permalink":"https://wizardtot.github.io/tags/%E5%AE%9E%E9%AA%8C/"}],"author":"WizardTOT"},{"title":"MapReduce总结","slug":"MapReduce总结","date":"2025-01-17T03:25:21.000Z","updated":"2025-05-31T08:16:27.217Z","comments":true,"path":"2025/01/17/MapReduce总结/","permalink":"https://wizardtot.github.io/2025/01/17/MapReduce%E6%80%BB%E7%BB%93/","excerpt":"开个新坑，这是mit6.824分布式系统的总结帖，方便自己复习。第一篇论文就是来自 Google 的 MapReduce，本篇总结帖包含简单的论文阅读和lab1算法实现。","text":"开个新坑，这是mit6.824分布式系统的总结帖，方便自己复习。第一篇论文就是来自 Google 的 MapReduce，本篇总结帖包含简单的论文阅读和lab1算法实现。 论文总结论文原文https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf 论文背景编写特定功能的分布式软件处理大规模的数据比较费时，能否设计一种分布式框架使得普通工程师也可以很容易的完成并运行大规模的分布式运算，在这种想法下 MapReduce 应运而生。 MapReduce 的想法来自于函数式编程语言中的 map 和 reduce ，用户仅需编写适当的 map 和 reduce 函数即可通过 MapReduce 接口轻松完成对大规模任务的并行计算。 MapReduce基本机制 首先输入的文件将分为 M 个 split 对应 map 任务的数量，然后会在集群上启动多个副本。 其中一个特殊的副本被称为 master，剩余的副本为 worker 由 master 管理并分配任务，总共由 M 个 map 任务以及 R 个 reduce 任务，master监测任务执行状态。 Map 阶段，map Worker 从 split 文件中解析 K&#x2F;V 对，传递给 map 函数执行，生成中间 K&#x2F;V 对缓存在内存中，缓存的中间 K&#x2F;V 对会定期写入的 worker 的磁盘。本地磁盘将分区函数划分为 R 个区域，磁盘的位置将通过 master 传递给 reduce worker。 进入 Reduce 阶段，reduce worker 会通过 RPC 调用访问 map worker 的本地缓存，读取完所有的中间数据后会排序，相同的键会分在一起。迭代排序结果，对于每一个唯一的中间键，会将 K&#x2F;V 传给 reduce 函数执行，reduce 函数将输出 append 到对应分区的最终文件中。 所有的 Map 任务和 Reduce 任务完成后，最终一致的结果（R 个分区文件）将返回给应用程序。通常 MapReduce 框架无需合并，这些结果文件会传送到下一个 MapReduce 或者能处理分块文件的分布式应用。 Master 会保存所有任务的状态(idle,in-progress,completed)并监控，分配给所有 worker 执行的任务，以及由 map 任务生成的 R 个中间文件的位置、大小。当 map 任务完成后更新的位置信息将递增推送个 reduce worker。 容错性MapReduce 的容错性由以下方面保证 Worker 故障Master 会定期 ping 每一个 worker，如果 worker 超时将会标记为failed。此时将会进行以下处理： 对于 completed 即已完成的 map 任务，将会标记为 idle 并分配给其他可执行的 worker，对于 in-progress 的map和reduce 任务同样进行以上处理。 对于 completed 的 reduce 任务，并不需要 Redo 由于存储到全局文件系统中。 当一个 map 任务因 WorkerA 失败转而由 WorkerB 执行，所有 reduce Worker 会收到重新执行的通知，任何尚未从 Worker A 读取数据的 reduce 任务将从 Worker B 读取数据。 Master 故障Master 故障可以通过 checkpoint 恢复或者重新执行 MapReduce 解决。 故障时的语义当提供的函数是输入值的确定性函数时（即对相同的输入值会得到相同的结果），MapReduce 保正并发执行时结果与顺序执行一致。论文中称使用一种名为原子提交机制 Atomic Commits 实现。 Map 任务的输出处理：每个正在进行的 map 任务会将其输出写到私有临时文件中。一个map任务会产生R个这样的文件，当 map 任务完成时，Worker 通知 Master，并在消息中包含 R 个临时文件的名称。当 Master 收到一个 map 任务首次完成消息时，它将那 R 个文件名保存在 Master 数据结构中 Reduce 任务的输出处理：当reduce任务完成时，reduce 的 worker 会原子性地将其临时输出文件重命名为最终输出文件。 性能优化备份worker落后的worker 会降低 MapReduce 的性能，当 MapReduce 操作接近完成时，master 可以通过备份 worker 安排给仍在 in-progress 的任务，只要有一个 worker 完成该任务即标记为 completed。 任务粒度将 map 阶段分为 M 个部分，将 reduce 任务分为 R 个部分，M 和 R 应当远大于 worker 的数量，可以使得负载平衡并且更坏恢复失败的任务。但实际中 M 和 R 的大小有限制，由于 master 要在 O(M+R) 次调度中保持 O(M*R) 种状态，此外 R 的数量还要受到用户程序的限制。 定制函数在 partition 时可以通过用户指定的哈希函数实现指定的功能，并且也可以在执行 reduce 前指定 combiner 函数先进行一轮合并再发送。 lab1算法实现lab1 无论是实现还是 debug 相比于后面的 lab 不过是些许风霜了。与论文的鸿沟不大。 coordinatorcoordinator 就是论文中的master，要实现的基本功能有： 记录任务相关的信息 响应 worker 对任务的请求 监测任务执行的状态 我定义的 coordinator 结构体如下, 其中 mutex 是 coordinator 的锁，会涉及多个 worker 的并发请求，cond 是控制任务分配的条件变量。 1234567891011type Coordinator struct &#123; // Your definitions here. mutex sync.Mutex cond *sync.Cond nMap int nReduce int MapTasks []*Task ReduceTasks []*Task mapPhaseDone bool reducePhaseDone bool&#125; 遍历任务列表寻找没有执行的任务分配任务给 worker ，如果没有则设置条件变量等待有空闲任务，lab中要求等待所有的 map 完成后才能分配 reduce 任务，这样实现就简单很多。 12345678910111213141516171819202122232425262728293031func (c *Coordinator) PollTask(args *TaskArgs, reply *Task) error &#123; c.mutex.Lock() defer c.mutex.Unlock() for &#123; if c.MapPhaseDone() &#123; break &#125; for _, task := range c.MapTasks &#123; if task.TaskState == Waiting &#123; *reply = *task c.TaskBegin(task) return nil &#125; &#125; c.cond.Wait() &#125; for &#123; if c.ReducePhaseDone() &#123; break &#125; for _, task := range c.ReduceTasks &#123; if task.TaskState == Waiting &#123; *reply = *task c.TaskBegin(task) return nil &#125; &#125; c.cond.Wait() &#125; return nil&#125; TaskBegin监测任务执行，对每一个正在执行的任务开一个线程，若10s中任务未完成就设置为 Waiting 状态交给其他的 worker 执行，这样就无需心跳机制。 12345678910111213141516171819func (c *Coordinator) TaskBegin(task *Task) &#123; task.TaskState = Working go func(task *Task) &#123; timedue := time.After(10 * time.Second) &lt;-timedue c.mutex.Lock() defer c.mutex.Unlock() if task.TaskState == Working &#123; if task.TaskType == MapTask &#123; log.Printf(&quot;Recover the map task %d&quot;, task.TaskID) &#125; if task.TaskType == ReduceTask &#123; log.Printf(&quot;Recover the reduce task %d&quot;, task.TaskID) &#125; task.TaskState = Waiting c.cond.Broadcast() &#125; &#125;(task)&#125; 任务执行完后 worker 要及时通过 RPC 更新状态。 1234567891011121314151617func (c *Coordinator) TaskDone(args *TaskDoneArgs, reply *TaskDoneReply) error &#123; c.mutex.Lock() defer c.mutex.Unlock() if args.TaskType == MapTask &#123; c.MapTasks[args.TaskID].TaskState = Done if c.MapPhaseDone() &#123; c.cond.Broadcast() &#125; &#125; if args.TaskType == ReduceTask &#123; c.ReduceTasks[args.TaskID].TaskState = Done if c.ReducePhaseDone() &#123; c.cond.Broadcast() &#125; &#125; return nil&#125; 剩余具体的任务执行完成检查仅需遍历任务列表，记得上锁即可。 RPC在 RPC 中我们需要定义一些 coordinator 与 worker 之间通信变量，有关任务的状态，任务的类型。 12345678910111213141516171819202122232425262728293031type Task struct &#123; TaskID int Nreduce int Nmap int FileName string TaskType TaskType TaskState TaskState&#125;type TaskArgs struct&#123;&#125;type TaskType intconst ( MapTask TaskType = iota ReduceTask)type TaskState intconst ( Working TaskState = iota Waiting Done)type TaskDoneArgs struct &#123; TaskID int TaskType TaskType&#125;type TaskDoneReply struct&#123;&#125; Worker与论文中分配任务稍有不同，Worker 执行时需要不断Pull 空闲的任务然后执行对应的操作。Worker 主要的流程就是获得任务，根据任务类型执行 map 函数或者 reduce 函数，执行完成后将更改任务状态。 具体实现通过 RPC 调用 Coordinator 中已经实现的接口。 12345678910111213141516func CallTask() (*Task, bool) &#123; args := TaskArgs&#123;&#125; reply := Task&#123;&#125; ok := call(&quot;Coordinator.PollTask&quot;, &amp;args, &amp;reply) return &amp;reply, ok&#125;func CallTaskDone(taskID int, taskType TaskType) (*TaskDoneReply, bool) &#123; args := TaskDoneArgs&#123; TaskID: taskID, TaskType: taskType, &#125; reply := TaskDoneReply&#123;&#125; ok := call(&quot;Coordinator.TaskDone&quot;, &amp;args, &amp;reply) return &amp;reply, ok&#125; 对于 Map 任务，对文件执行完 mapFunc 后，创建类型为 [][]KeyValue 的kvmap进行存储，根据 reduce 的数量将键值分配，将 kvmap 编码后写入名为mr-%d-%d的文件中，第一个代表任务id，第二个代表 reduce 划分。 123456789101112131415161718192021222324252627282930313233func DoMapTask(task *Task, mapFunc func(string, string) []KeyValue) &#123; file, err := os.Open(task.FileName) //fmt.Printf(&quot;Task %d open file %s\\n&quot;, task.TaskID, task.FileName) if err != nil &#123; log.Fatalf(&quot;Maptask %d cannot open %v&quot;, task.TaskID, task.FileName) &#125; content, err := io.ReadAll(file) if err != nil &#123; log.Fatalf(&quot;cannot read %v&quot;, task.FileName) &#125; file.Close() kva := mapFunc(task.FileName, string(content)) kvmap := make([][]KeyValue, task.Nreduce) for _, kv := range kva &#123; i := ihash(kv.Key) % task.Nreduce kvmap[i] = append(kvmap[i], kv) &#125; for i, ikva := range kvmap &#123; intermediateName := fmt.Sprintf(&quot;mr-%d-%d.m&quot;, task.TaskID, i) tmpfile, err := os.CreateTemp(&quot;./&quot;, &quot;mr-*.tmp&quot;) if err != nil &#123; log.Fatal(&quot;Can&#x27;t create&quot;) &#125; encoder := json.NewEncoder(tmpfile) for _, kv := range ikva &#123; encoder.Encode(kv) &#125; tmpfile.Close() os.Rename(tmpfile.Name(), intermediateName) // fmt.Printf(&quot;MapTask %d create file %s\\n&quot;, task.TaskID, intermediateName) &#125; CallTaskDone(task.TaskID, task.TaskType)&#125; 执行reduce任务，读取中间文件，将所有的键值对按照键排序，将相同键的值通过 reduceFunc 进行处理，结果写入 mr-out-%d 中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func DoReduceTask(task *Task, reduceFunc func(string, []string) string) &#123; intermediate := []KeyValue&#123;&#125; for i := 0; i &lt; task.Nmap; i++ &#123; intermediateName := fmt.Sprintf(&quot;mr-%d-%d.m&quot;, i, task.TaskID) file, err := os.Open(intermediateName) if err != nil &#123; log.Fatalf(&quot;Can&#x27;t open file %v&quot;, intermediate) &#125; decoder := json.NewDecoder(file) for &#123; var kv KeyValue if err := decoder.Decode(&amp;kv); err != nil &#123; if err == io.EOF &#123; break &#125; log.Fatalf(&quot;Can&#x27;t decode %v&quot;, intermediateName) &#125; intermediate = append(intermediate, kv) &#125; file.Close() &#125; sort.Sort(ByKey(intermediate)) //fmt.Printf(&quot;%v\\n&quot;, intermediate) outputFileName := fmt.Sprintf(&quot;mr-out-%d&quot;, task.TaskID) tmpfile, err := os.CreateTemp(&quot;./&quot;, &quot;mr-*.tmp&quot;) if err != nil &#123; log.Fatal(&quot;Can&#x27;t create&quot;) &#125; i := 0 for i &lt; len(intermediate) &#123; j := i + 1 for j &lt; len(intermediate) &amp;&amp; intermediate[j].Key == intermediate[i].Key &#123; j++ &#125; values := []string&#123;&#125; for k := i; k &lt; j; k++ &#123; values = append(values, intermediate[k].Value) &#125; output := reduceFunc(intermediate[i].Key, values) fmt.Fprintf(tmpfile, &quot;%v %v\\n&quot;, intermediate[i].Key, output) i = j &#125; tmpfile.Close() os.Rename(tmpfile.Name(), outputFileName) // fmt.Printf(&quot;ReduceTask %d create file %s \\n&quot;, task.TaskID, outputFileName) CallTaskDone(task.TaskID, task.TaskType)&#125; 关于debug，总体而言这个 lab 基本不会有活锁或者不一致的问题考虑，注意锁的使用即可。","categories":[{"name":"MIT6.824","slug":"MIT6-824","permalink":"https://wizardtot.github.io/categories/MIT6-824/"}],"tags":[{"name":"MIT6.824","slug":"MIT6-824","permalink":"https://wizardtot.github.io/tags/MIT6-824/"}],"author":"WizardTOT"},{"title":"PA0-1","slug":"PA0-1","date":"2024-07-06T05:30:53.000Z","updated":"2025-05-31T08:16:06.644Z","comments":true,"path":"2024/07/06/PA0-1/","permalink":"https://wizardtot.github.io/2024/07/06/PA0-1/","excerpt":"The machine is always right! Every line of untested code is always wrong.","text":"The machine is always right! Every line of untested code is always wrong. PA1快来玩呀https://jyywiki.cn/ICS/2021/labs/PA1.html在这个链接中下载各种游戏的.nes文件,参考fceux-am&#x2F;README.md文件中的内容，将游戏ROM放置在nes/rom/目录下, 并命名为xxx.nes, 如nes/rom/mario.nes.然后可通过mainargs选择运行的游戏, 如: 1make ARCH=native run mainargs=mario 然后就可以开始玩啦,我也就玩了一个下午。 关于加快编译速度使用ccahe工具可以将第一次编译的目标文件存放于缓存中，下次编译时若源文件没有变化则直接取出之前的目标文件作为目标结果。这里提供一个快速配置的方法，安装完ccache后，打开.bashrc文件，然后添加如下内容： 12#ccacheexport PATH=&quot;/usr/lib/ccache:$PATH&quot; 再执行source .bashrc后，执行which gcc可得到如下结果： 1/usr/lib/ccache/gcc 之后再进行编译时速度就加快啦 配置vscode直接使用GDB也行，当然在vscode中编译调试更方便，首先是c_cpp_properties.json,将编译文件所需的宏添加即可，不需要其他的改动 12345678910111213141516171819202122&#123; &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;Linux&quot;, &quot;includePath&quot;: [ &quot;$&#123;workspaceFolder&#125;/**&quot; ], &quot;defines&quot;: [ &quot;__GUESR_ISA__=riscv32&quot;, &quot;_GNU_SOURCE&quot;, &quot;__STDC_CONSTANT_MACROS&quot;, &quot;__STDC_FORMAT_MACROS&quot;, &quot;__STDC_LIMIT_MACROS&quot; ], &quot;compilerPath&quot;: &quot;/usr/lib/ccache/clang&quot;, &quot;cStandard&quot;: &quot;c17&quot;, &quot;cppStandard&quot;: &quot;c++14&quot;, &quot;intelliSenseMode&quot;: &quot;linux-clang-x64&quot; &#125; ], &quot;version&quot;: 4&#125; 调试要记得在build.mk中添加-g选项，然后按如下内容配置launch.json 1234567891011121314151617181920212223242526272829303132 // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123;&quot;name&quot;: &quot;(gdb) Launch&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;$&#123;workspaceRoot&#125;/nemu/build/riscv32-nemu-interpreter&quot;, &quot;args&quot;: [&quot;-b&quot;], &quot;stopAtConnect&quot;: false, &quot;cwd&quot;: &quot;$&#123;workspaceRoot&#125;/nemu&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;setupCommands&quot;: [ &#123; &quot;description&quot;: &quot;为 gdb 启用整齐打印&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true &#125;, &#123; &quot;description&quot;: &quot;将反汇编风格设置为 Intel&quot;, &quot;text&quot;: &quot;-gdb-set disassembly-flavor intel&quot;, &quot;ignoreFailures&quot;: true &#125; ], &quot;miDebuggerPath&quot;: &quot;/usr/bin/gdb&quot;, &#125; ]&#125; 还有tasks.json 1234567891011121314151617181920212223242526272829303132333435&#123; &quot;tasks&quot;: [ &#123; &quot;label&quot;: &quot;make&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;make -j8&quot;, &quot;problemMatcher&quot;: [], &quot;group&quot;: &quot;build&quot; &#125;, &#123; &quot;type&quot;: &quot;cppbuild&quot;, &quot;label&quot;: &quot;C/C++: gcc 生成活动文件&quot;, &quot;command&quot;: &quot;/usr/bin/gcc&quot;, &quot;args&quot;: [ &quot;-fdiagnostics-color=always&quot;, &quot;-g&quot;, &quot;$&#123;file&#125;&quot;, &quot;-o&quot;, &quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;&quot; ], &quot;options&quot;: &#123; &quot;cwd&quot;: &quot;$&#123;fileDirname&#125;&quot; &#125;, &quot;problemMatcher&quot;: [ &quot;$gcc&quot; ], &quot;group&quot;: &#123; &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true &#125;, &quot;detail&quot;: &quot;调试器生成的任务。&quot; &#125; ], &quot;version&quot;: &quot;2.0.0&quot;&#125; 优美的退出为了测试大家是否已经理解框架代码, 我们给大家设置一个练习: 如果在运行NEMU之后直接键入q退出, 你会发现终端输出了一些错误信息. 请分析这个错误信息是什么原因造成的, 然后尝试在NEMU中修复它. 在RTFSC后对于这个问题主要关注sdb_mainloop()函数，这是简易调试器的主循环， void sdb_mainloop() &#123; if (is_batch_mode) &#123; cmd_c(NULL); return; &#125; for (char *str; (str = rl_gets()) != NULL; ) &#123; char *str_end = str + strlen(str); /* extract the first token as the command */ char *cmd = strtok(str, &quot; &quot;); if (cmd == NULL) &#123; continue; &#125; /* treat the remaining string as the arguments, * which may need further parsing */ char *args = cmd + strlen(cmd) + 1; if (args &gt;= str_end) &#123; args = NULL; &#125; #ifdef CONFIG_DEVICE extern void sdl_clear_event_queue(); sdl_clear_event_queue(); #endif int i; for (i = 0; i &lt; NR_CMD; i ++) &#123; if (strcmp(cmd, cmd_table[i].name) == 0) &#123; if (cmd_table[i].handler(args) &lt; 0) &#123; return; &#125; break; &#125; &#125; if (i == NR_CMD) &#123; printf(&quot;Unknown command &#39;%s&#39;\\n&quot;, cmd); &#125; &#125; &#125; 这个函数首先检查是否为批处理模式，若is_batch_mode为1则执行默认的cmd_c(NULL),在调试时记得设置is_batch_mode为false。然后读取输入的字符串，遍历cmd_table数组，找到与输入命令匹配的命令处理函数，将args传递给对应的函数处理，在调试时可以看到执行cmd_q()后调用is_exit_status_bad()。 NEMUState nemu_state = &#123; .state = NEMU_STOP &#125;; int is_exit_status_bad() &#123; int good = (nemu_state.state == NEMU_END &amp;&amp; nemu_state.halt_ret == 0) || (nemu_state.state == NEMU_QUIT); return !good; &#125; 可以发现nemu_state的默认值为NEMU_STOP,直接运行q退出时nemu的状态未改变，在cmd_q()中添加nemu_state.state&#x3D;NEMU_QUIT即可 基础设施实现单步执行, 打印寄存器, 扫描内存 单步执行的实现很简单，让程序单步执行N条指令后暂停执行,当N没有给出时, 缺省为1 static int cmd_si(char *args)&#123; if(args == NULL)&#123; cpu_exec(1); return 0; &#125; int step; sscanf(args,&quot;%d&quot;,&amp;step); cpu_exec(step); return 0; &#125; 打印寄存器，输入info r时打印寄存器的状态，需要实现reg.c中isa_reg_display() static int cmd_info(char *args)&#123; if(args == NULL)&#123; printf(&quot;Please enter specifically&quot;); return 0; &#125; char *arg = strtok(args,&quot; &quot;); if(strtok(NULL,&quot; &quot;))&#123; printf(&quot;Too many parameters&quot;); return 0; &#125; if(strcpy(arg,&quot;r&quot;)==0)&#123; isa_reg_display(); &#125; return 0; &#125; 扫描内存的实现也不难, 对命令进行解析之后, 先求出表达式的值. 但你还没有实现表达式求值的功能, 现在可以先实现一个简单的版本: 规定表达式EXPR中只能是一个十六进制数,x N EXPR，EXPR作为起始内存地址, 以十六进制形式输出连续的N个4字节。注意读内存中数据使用paddr_read() static int cmd_x(char *args)&#123; char* n = strtok(args,&quot; &quot;); char* base = strtok(NULL,&quot; &quot;); int len = 0; paddr_t addr = 0; sscanf(n, &quot;%d&quot;, &amp;len); sscanf(base,&quot;%x&quot;, &amp;addr); for(int i = 0 ; i &lt; len ; i ++) &#123; printf(&quot;%x\\n&quot;,paddr_read(addr,4)); addr = addr + 4; &#125; return 0; &#125; 我在 日落到来 遥望太阳 潜入深海 等待 季节更改 年复一年 行程没改 尽管被夜取代 不表示光不存在 19 岁时我尝试拥抱未知，在20岁的新起点我希望投入更多时间留给 passion","categories":[{"name":"PA","slug":"PA","permalink":"https://wizardtot.github.io/categories/PA/"}],"tags":[{"name":"PA","slug":"PA","permalink":"https://wizardtot.github.io/tags/PA/"}],"author":"WizardTOT"},{"title":"西电计组实验攻略","slug":"西电计组实验攻略","date":"2024-06-29T13:08:34.000Z","updated":"2025-01-18T05:08:53.744Z","comments":true,"path":"2024/06/29/西电计组实验攻略/","permalink":"https://wizardtot.github.io/2024/06/29/%E8%A5%BF%E7%94%B5%E8%AE%A1%E7%BB%84%E5%AE%9E%E9%AA%8C%E6%94%BB%E7%95%A5/","excerpt":"西电计科大二计算机组成与体系结构实验","text":"西电计科大二计算机组成与体系结构实验 关于实验大二下学期鄙人确实被计科的实验安排恶心到了。这些实验不难，但非常占用时间，学期末实验成绩也不错，为了造福以后的学妹学弟，特此将实验报告放底下，可以参考参考，但是不要直接抄袭。 实验攻略其实每个实验老师都还行，没有对策府库那么夸张。老师之间的要求有些不同，我的老师每次实验都会提出一些创新的要求，完成要求可以获得A或者A+，没有完成要求只能获得A-，但是如果直接使用demo会喜提B，也听说有做得快就能得高分的老师。 快速完成实验可以直接打开D盘找到前几批的遗产。 实验报告实验一 存储器实验【实验目的】 掌握 FPGA 中 lpm_ROM 的设置，作为只读存储器 ROM 的工作特性和配置方法。用文本编辑器编辑 mif 文件配置 ROM，学习将程序代码以 mif 格式文件加载于lpm_ROM 中；在初始化存储器编辑窗口编辑 mif 文件配置 ROM；验证 FPGA 中 mega_lpm_ROM 的功能。 【实验环境】 QuartusII、FPGA实验箱 【实验基本原理及步骤】 lpm_ROM 有 5 组信号：地址信号 address[ ]、数据信号 q[ ]、时钟信号 inclock、outclock、允许信号memenable，其参数都是可以设定的。由于 ROM 是只读存储器，所以它的数据口是单向的输出端口，ROM 中的数据是在对 FPGA 现场配置时，通过配置文件一起写入存储单元的。图 3-1-1 中的 lpm_ROM 有 3 组信号：inclk——输入时钟脉冲；q[23..0]——lpm_ROM 的 24 位数据输出端；a[5..0]——lpm_ROM 的 6 位读出地址。实验中主要掌握以下三方面的内容： lpm_ROM 的参数设置； lpm_ROM 中数据的写入，即 LPM_FILE 初始化文件的编写； lpm_ROM 的实际应用，在实验台上的调试方法。 实验原理图： 实验步骤如下： 用图形编辑，进入mega_lpm元件库，调用lpm_rom元件，设置地址总线宽度address[]和数据总线宽度q[]，分别为6位和8位,并添加输入输出引脚，如图设置和连接。 设置上图为工程。 在设置lpm_rom数据参数选择项lpm_file的对应窗口中，用键盘输入lpm_ROM配置文件的路径（rom_a.mif），然后设置在系统ROM&#x2F;RAM读写允许，以便能对FPGA中的ROM在系统读写。 用初始化存储器编辑窗口编辑lpm_ROM配置文件（文件名.mif）。这里预先给出后面将要用到的微程序文件：rom_a.mif 。rom_a.mif中的数据是微指令码。 全程编译。 下载SOF文件至FPGA，改变lpm_ROM的地址a[5..0]，外加读脉冲，通过实验台上的数码管比较读出的数据是否与初始化数据(rom_a.mif中的数据)一致。 【实验数据记录】 仿真波形图：设定的ROM数据如下： 【实验结果分析】 在波形仿真中可以看到设定的地址信号address是随着时钟周期顺序增加的，lpm_ROM对应地址存储的数据被相应读出。 将sof文件下载到FPGA实验箱后，改变ROM的地址a[5..0]，外加读脉冲，通过观察数码管的显示可以发现与mif文件设置的数据一致，本次实验验证了lpm_rom作为只读储存器，数据要一次性写入到rom中，在读取数据时要通过输入指定的地址值并在时钟信号的驱动下，lpm_rom会将输出地址对应的数据。 通过本次实验我对lpm_rom有了深入的理解，FPGA中lpm_rom由EBA构成，存储内容在配置的过程中写入的，一旦配置完成内容就固定，只能读取不能修改，这种只读特性使得lpm_rom适用于在计算机中存储固定数据和固件等需要保持不变的信息。 实验二 运算器实验【实验目的】 了解简单运算器的数据传输通路。验证运算功能发生器的组合功能。 掌握算术逻辑运算加、减、与的工作原理。验证实验台运算的 8 位加、减、与、直通功能。按给定数据，完成几种指定的算术和逻辑运算。 【实验环境】 QuartusII、FPGA实验箱 【实验基本原理及步骤】 算术逻辑单元 ALU 的数据通路如图 2-1 所示。其中运算器 ALU181 根据 74LS181 的功能用 VHDL 硬件描述语言编辑而成，构成 8 位字长的 ALU。参加运算的两个 8 位数据分别为 A[7..0]和 B[7..0]，运算模式由 S[3..0]的 16 种组合决定，而 S[3..0]的值由4 位 2 进制计数器 LPM_COUNTER 产生，计数时钟是 Sclk（图 2-1）；此外，设 M&#x3D;0，选择算术运算，M&#x3D;1 为逻辑运算，CN为低位的进位位；F[7..0]为输出结果，CO为运算后的输出进位位。两个 8 位数据由总线 IN[7..0]分别通过两个电平锁存器 74373 锁入。 实验步骤如下： 设计ALU元件在Quartus II 环境下，用文本输入编辑器Text Editor输入ALU181.VHD算术逻辑单元文件，编译VHDL文件，并将ALU181.VHD文件制作成一个可调用的原理图元件。 以原理图方式建立顶层文件工程选择图形方式。根据图2-1输入实验电路图，从Quartus II的基本元件库中将各元件调入图形编辑窗口、连线，添加输入输出引脚。将所设计的图形文件ALU.bdf保存到原先建立的文件夹中，将当前文件设置成工程文件，以后的操作就都是对当前工程文件进行的。 器件选择选择Cyclone系列，在Devices中选择器件EP1C6QC240C8。编译，引脚锁定，再编译。引脚锁定后需要再次进行编译，才能将锁定信息确定下来，同时生成芯片编程&#x2F;配置所需要的各种文件。 芯片编程Programming（可以直接选择光盘中的示例已完成的设计进行验证实验）打开编程窗口。将配置文件ALU.sof下载进GW48系列现代计算机组成原理系统中的FPGA中。 选择实验系统的电路模式是NO.0，验证ALU的运算器的算术运算和逻辑运算功能根据表2-1，从键盘输入数据A[7..0]和B[7..0]，并设置S[3..0]、M、Cy，验证ALU运算器的算术运算和逻辑运算功能，记录实验数据。 【实验数据记录】 电路设计图如下：仿真波形图如下：ALU181.VHL如下：【实验结果分析】 图一是本次实验的原理图，其中ALU181是运算器，是通过编译VHDL文件生成的原理图文件，74343B是8位的缓存器，用于暂存向ALU中输入的数据，A0_B1信号控制数据送入到哪个暂存器中，低电平送入到A暂存器中，高电平送入到B暂存器中，S信号控制ALU的运算模式（对应实验原理中的表格），M信号用于控制ALU执行算术运算还是逻辑运算，ALU的运算结果为F，在波形图中，输入到暂存器A的数据为AA，输入到暂存器B的数据为55，执行算术运算，对应的运算结果与ALU的运算模式一致，在FPGA实验箱中运算结果也一致。 通过本次实验我对CPU中的ALU部件有了更清晰的认知，ALU可执行多种算术运算和逻辑运算，ALU的工作流程是由控制单元控制的，接收来自寄存器中的数据，根据对应的控制信号执行特定的运算类型，ALU通过与其他部件协调工作，实现了计算机内部的高效运算和处理能力。 实验三 节拍脉冲发生器时序电路实验【实验目的】 掌握节拍脉冲发生器的设计方法和工作原理。 理解节拍脉冲发生器的工作原理。 【实验环境】 QuartusII、FPGA实验箱 【实验基本原理及步骤】 计算机之所以能够按照人们事先规定的顺序进行一系列的操作或运算，就是因为它的控制部分能够按一定的先后顺序正确地发出一系列相应的控制信号。这就要求计算机必须有时序电路。控制信号就是根据时序信号产生的。本实验说明时序电路中节拍脉冲发生器的工作原理。连续节拍发生电路：可由 4 个 D 触发器组成，可产生 4 个等间隔的时序信号 T1T4，其中 CLK1 为时钟信号，由实验台右边的方波信号源 clock0 提供，可产生 1Hz12MHz 的方波信号频率。实验者可根据实验自行选择信号频率。当 RST1 为低电平时，T1 输出为“1”，而 T2、T3、T4 输出为“0”；当 RST1 由低电平变为高电平后，T1~T4 将在CLK1 的输入脉冲作用下，周期性地轮流输出正脉冲，机器进入连续运行状态（EXEC）。单步节拍发生电路：该电路每当 RST1 出现一个负脉冲后，仅输出一组 T1、T2、T3、T4 节拍信号，直到 RST1 出现下一个负脉冲单步&#x2F;连续节拍发生电路:增加两个 2-1 多路选择器，可将图单步节拍发生电路改变为单步&#x2F;连续节拍发生电路。S0 是单步或连续节拍发生控制信号，当 S0&#x3D;0，选择单步运行方式；当 S0&#x3D;1, 选择连续运行方式。【实验数据记录】 本次实验在四节拍的单步&#x2F;连续节拍发生电路基础上，增加节拍数至八位，电路设计图如下：仿真波形图如下：【实验结果分析】图一是本次实验的原理图，节拍脉冲发生器是通过9个D触发器级联实现的，其中T1-T8是脉冲信号，CLK1是时钟信号，RST1是置零信号，S0是控制脉冲是单步还是连续的信号，单步运行只在复位后 T1-T8 依次产生一个脉冲，对应与计算机中执行一条指令，而连续运行复位后 T1-T8 将依次产生脉冲不断循环下去。在波形仿真中可以看到，当RST1低位的时候脉冲信号置零，RST1为高位时脉冲顺序发生，在实验箱中验证，显示了八位的脉冲信号成功生成并可控制。 通过本次实验我理解了节拍脉冲发生器的工作机制，节拍脉冲发生器通过产生节拍信号协调计算机中各个部件之间运作，是计算机中重要的时序控制器件，它能控制发生单步脉冲和连续脉冲，用于控制单步指令的执行还是连续指令程序的运行。 问题回答： 单步运行与连续运行有何区别，它们各自的使用环境怎样？ 单步运行只在复位后 T1-T8 依次产生一个脉冲，而连续运行复位后 T1-T8 将依次产生脉冲不断循环。单步脉冲对应指令的逐条、单步的执行，这种模式允许逐条地观察指令执行的结果和变化，从而进行调试和分析。单步运行特别适用于调试程序中的错误和异常情况，连续脉冲对应指令的连续运行，指程序的执行没有中断，按顺序执行所有的指令直到程序结束或者遇到中断信号。在连续运行模式下，计算机会自动执行指令。 如何实现单步&#x2F;连续运行工作方式的切换？ 增加两个 2-1 多路选择器，单步节拍发生电路改变为单步/连续节拍发生电路。通过输入信号控制单步或连续节拍。 实验四 程序计数器PC与地址寄存器AR实验【实验目的】 掌握地址单元的工作原理。掌握的两种工作方式，加 1 计数和重装计数器初值的实现方法；掌握地址寄存器从程序计数器获得数据和从内部总线获得数据的实现方法。【实验环境】 QuartusII、FPGA实验箱 【实验基本原理及步骤】 实验原理： 地址单元主要由三部分组成：程序计数器、地址寄存器和多路开关。程序计数器 PC 用以指出下一条指令在主存中的存放地址，CPU 正是根据 PC 的内容去存取指令的。因程序中指令是顺序执行的，所以 PC 有自增功能。程序计数器提供下一条程序指令的地址，如电路图 4-2-1 所示，在 T4 时钟脉冲的作用下具有自动加 1 的功能；在 LDPC 信号的作用下可以预置计数器的初值（如子程序调用或中断响应等）。当 LDPC 为高电平时，计数器装入 data[ ]端输入的数据。aclr 是计数器的35清 0 端，高电平有效（高电平清零）；aclr 为低电平时，允许计数器正常计数。地址寄存器 AR（74273）锁存访问内存 SRAM 的地址。273 中的地址来自两个渠道。一是程序计数器 PC 的输出，通常是下一条指令的地址；二是来自于内部数据总线的数据，通常是被访问操作数的地址。为了实现对两路输入数据的切换，在 FPGA 的内部通过总线多路开关 BUSMUX 进行选择。LDAR 与多路选择器的 sel 相连，当 LDAR 为低电平，选择程序计数器的输出；当 LDAR 为高电平时，选择内部数据总线的数据。实验步骤： 按照 图 4-2-1 程序计数器原理图编辑、输入电路，实验台选择 NO.0 工作模式。对输入原理图进行编译、引脚锁定、并下载到实验台。示例工程文件是PC_unit.bdf。硬件实验验证（与仿真波形图 4-2-2 比较！）。实验说明： 下载 pc_unit.sof ； 用模式键选模式“0”，再按一次右侧的复位键； 键 2 和键 1 可输入 8 位总线数据 B7..0；CLR（键 5）按 2 次(0à1à0)，产生一正脉冲，高电平清零；LDAR（键 6）&#x3D;0 时，BUSMUX 输出程序计数器PC 的值；LDAR&#x3D;1 时，BUSMUX 输出 B[7..0]总线数据。LDPC（键 7）：程序计数器 PC 预置控制端，当 LDPC&#x3D;1 时，将 B[7..0]总线数据装入程序计数器PC；当 LDPC&#x3D;0 时，程序计数器 PC 处于计数自动工作状态，对 T4 进行计数；T4（键 8）：程序计数器 PC 的计数时钟 CLK，键 8 按动两次产生一个计数脉冲。 通过 B[7..0]设置程序计数器的预加载数据。当 LDPC&#x3D;0 时，观察程序计数器自动加 1 的功能；当 LDPC&#x3D;1 时，观察程序计数器加载输出情况。 示例操作： 所有键置 0，键 2&#x2F;1 输入 A5；按键 5àPC 计数器清 0(0à1à0)； 连续按动键 8，可以从数码 8&#x2F;7 上看到 AR 的输出，即 PC 值； 按键 6-&gt;’1’，选通直接输出总线上的数据 A5 作为 PC 值，按键 8，产生一个脉冲上升沿，即可看到 AR（显示在数码 8&#x2F;7）的输出为 A5； 使键 6&#x3D;0，仍选通 PC 计数器输出，这时键 2&#x2F;1 输入 86，按键 7 产生一个上升脉冲，即用 LDPC 将 86 加载进 PC 计数器 【实验数据记录】 电路图设计如下：仿真波形如下：【实验结果分析】 本次实验在原有实验基础上将数据从8位增加至12位，并对程序计数器增添自增和自减控制，由updown信号控制，当updown为低电平时，程序计数器自增，updown为高电平时，程序计数器自减。地址寄存器可以从程序计数器获得数据和从内部总线获得数据。LDPC信号用于控制程序计数器初值，LDPC为高电平时将输入的数据存入程序计数器中。为了实现对两路输入数据的切换，在 FPGA 的内部通过总线多路开关lpm_mux进行选择，LDAR 与多路选择器的 sel 相连，当 LDAR 为低电平，选择程序计数器的输出，当 LDAR 为高电平时，选择内部数据总线的数据。这些功能均在波形仿真和实验箱中得到验证。 通过本次实验我理解了计算机中的地址单元，计算机中的地址单元程序计数器 PC 用以指出下一条指令在主存中的存放地址，程序大部分情况是顺序执行的，因此PC通常自增，而在转移、中断或者子程序调用时PC需要加载其他指令的地址，地址寄存器AR用于存放主存地址，访问主存需首先将地址存入AR中，程序计数器、地址寄存器和多路选择组成了计算机中的地址单元，控制访问主存的地址。 问题回答： 分支和转移程序与顺序程序有何区别？要实现程序的分支和转移，需要对程序计数器PC和地址寄存器AR作怎样的操作？应改变哪些控制信号？ 分支和转移程序与顺序程序的主要区别在于执行流程。顺序程序按照预定义的指令序列逐条执行，而分支和转移程序可以根据条件或指令来改变执行流程，跳转到程序的其他部分执行。要实现程序的分支和转移，通常需要对程序计数器（PC）和地址寄存器（AR）进行操作。需要计算转移地址，更新PC的值以指向新的目标地址。在某些情况下，可能还需要更新AR的值以指向正确的内存位置。 实验五 总线控制实验【实验目的】 理解总线的概念及特性；掌握总线传输控制特性。 【实验环境】 QuartusII、FPGA实验箱 【实验基本原理及步骤】 1．总线的基本概念 总线是多个系统部件之间进行数据传输的公共通路，是构成计算机系统的骨架。借助总线连接，计算机在系统各部件之间实现传送地址、数据和控制信息的操作。所谓总线就是指能为多个功能部件服务的一组公用信息线。2．实验原理 实验所用总线实验传输框图如图 5-1 所示。它将几种不同的设备挂在总线上，有存储器、输入设备、输出设备、寄存器。这些设备在传统的系统中需要有三态输出控制，然而在 FPGA 的内部没有三态输出控制结构，因此必须采用总线输出多路开关结构加以控制。按照传输要求恰当有序地控制它们，就可以实现总线信息传输。 实验内容1 根据图5-2完成实验操作：选择实验模式“0”；再按一次右侧的复位键(用一接线将实验板上键9的输入端插针与适配板上FPGA的第P196针相连，以便能用键9控制OUT锁存器的时钟；)：初始状态； 键4、键3控制设备选择端：sel[1..0]&#x3D;00（键4&#x2F;键3&#x3D;00，）； 此时由键2&#x2F;键1输入的数据(26H,显示于数码管2&#x2F;1)直接进入BUS（数码管8&#x2F;7显示），键5、6、7为低电平； 键8&#x3D;1（允许RAM写入）完成图5-2所示的操作： 键5发正脉冲（0-1-0），将数据打入寄存器R0； 键2&#x2F;键1再输入数据(如37H)； 键6发正脉冲（0-1-0），将数据打入地址寄存器AR； 键2&#x2F;键1再输入数据(如48H)； 键7发正脉冲（0-1-0），将数据写入RAM（此时必须键8输出‘1’，注意此时进入RAM的数据48H是放在地址37H单元的）； 键2&#x2F;键1再输入数据(如59H)； 键9发正脉冲（0-1-0），将数据写入寄存器OUT（数码管6&#x2F;5将显示此数）； 键4、键3分别选择sel[1..0]&#x3D;00、01、10、11，从数码管8&#x2F;7上观察被写入的各寄存器中的数据。 实验内容2： 先将数据28H写入RAM的地址（4AH），再将数据1BH送进R0，最后将刚才写入RAM中地址（4AH）的数据读出送到OUT口。依据总线电路图5-3，操作如下： 用一接线将实验板上键9的输入端插针与适配板上FPGA的第P196针相连，以便能用键9控制OUT锁存器的时钟；键3、4、5、6、7、8都为低电平，使键4&#x2F;键3&#x3D;00，即总线多路选择器sel[1..0]&#x3D;00，选择由键2&#x2F;键1输入的数据4AH（地址），直接进入BUS； 按键6两次（0-1-0），产生一个正脉冲，将地址数据4AH（地址）锁入地址寄存器AR，如图5-3所示，此数据直接进入RAM的address端； 按键2&#x2F;键1，输入数据28H（数据），此时直接进入总线BUS，并进入RAM的data数据端；按键8&#x3D;1（RAM写允许）；按键7两次，将数28H写入RAM（地址为4AH），最后按键8&#x3D;0，写禁止，读允许。 由键2&#x2F;键1输入的数据1BH，按键5两次（0-1-0），产生一个正脉冲，即此数写入R0寄存器。 读RAM送到OUT：由键2&#x2F;键1输入的数据4AH，按键5两次，使4AH进入AR； 按键7两次，RAM中4AH单元中的数据28H输出，再使键4&#x2F;键3&#x3D;10，即总线多路选择器sel[1..0]&#x3D;10，此时RAM数据口的28H进入总线BUS（可从数码管8&#x2F;7上看到）； 按键9一次（此键是单脉冲），RAM口的28H即被锁如输出口OUT寄存器，由数码管6&#x2F;5显示。 图5-2 总线数据传输练习操作步骤 键盘&#x2F;显示定义详细说明：1）键2、键1输入D[7..0]，输入的数据同时显示在数码2和数码3上。2）键9、键3输入控制设备选择端sel[1..0]，如图5-2所示，键4&#x2F;键3控制总线多路选择器，选择不同设备的数据进入总线：sel[1..0]&#x3D; 00：输入设备INPUT数据进入总线BUS；&#x3D; 01：寄存器R0中的数据进入总线BUS；&#x3D; 11：地址寄存器AR的数据进入总线BUS；&#x3D; 10：存储器RAM的数据进入总线BUS；4）总线BUS上的输出数据显示在数码8和数码7上；5）键5控制寄存器R0的输入选通锁存端；6）键6控制地址寄存器AR输入选通锁存端；7）键7控制LPM_RAM数据DATA输入锁存端；8）键8控制LPM_RAM写入允许WE端，&#x3D;1有效； 9）键9控制输出设备OUTPUT的输入选通端，输出数据显示在数码6和数码5上，要求首先用一接线将实验板上键9的输入端插针与适配板上FPGA的第P196针相连。 【实验数据记录】 电路图如下：波形仿真如下：【实验结果分析】 在波形仿真中可以看到，LDAR信号高电平时将输入数据存入地址寄存器，这时存入的是90，当s信号为10时，地址寄存器的数据写入总线，可以看到总线的数据为90，与地址寄存器数据一致。当LDR0为高电平时，将输入数据存入寄存器R0中，这时存入的数据是48，当s信号为01时，R0的数据写入总线，可以看到总线的数据为48，与R0的数据一致。本次实验成功经过波形仿真和实验箱验证。 本次实验通过总线与寄存器 R0、AR、RAM 相连，模拟了CPU中的数据通路，实现了输入数据的保存与输出。这次实验让我更加清晰地理解了总线，计算机的总线可以划分为数据总线、地址总线和控制总线，分别用来传输数据、数据地址和控制信号。总线是一种内部结构，它是 cpu、内存、输入、输出设备传递信息的公用通道，主机的各个部件通过总线相连接，外部设备通过相应的接口电路再与总线相连接，从而形成了计算机硬件系统。 问题回答： 如何向 RAM 中输入多个数据，并在输出设备 OUTPUT 上显示这些数据？ 将数据输入到RAM的不同地址中，先输入地址到地址寄存器中，再将数据写入到RAM的数据锁存端中，将数据写入到该地址对应的区域，随后按此步骤再输入多个地址和数据存入RAM，显示数据时，只需要将地址输入到地址寄存器中，读出RAM中对应的数据即可。 传输过程中是否会在总线上发生数据冲突？若发生冲突应怎样避免？ 可能会发生冲突，解决方法： 加入总线仲裁器，每个需要占用总线的设备需发送总线请求信号到仲裁器中，由仲裁器决定哪个设备获得对总线的使用权。","categories":[{"name":"实验","slug":"实验","permalink":"https://wizardtot.github.io/categories/%E5%AE%9E%E9%AA%8C/"}],"tags":[{"name":"实验","slug":"实验","permalink":"https://wizardtot.github.io/tags/%E5%AE%9E%E9%AA%8C/"}],"author":"WizardTOT"},{"title":"Hello World","slug":"hello-world","date":"2024-06-27T11:40:32.268Z","updated":"2024-07-04T03:50:49.760Z","comments":true,"path":"2024/06/27/hello-world/","permalink":"https://wizardtot.github.io/2024/06/27/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[],"author":"WizardTOT"}],"categories":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://wizardtot.github.io/categories/TinyKV/"},{"name":"MIT6.824","slug":"MIT6-824","permalink":"https://wizardtot.github.io/categories/MIT6-824/"},{"name":"实验","slug":"实验","permalink":"https://wizardtot.github.io/categories/%E5%AE%9E%E9%AA%8C/"},{"name":"PA","slug":"PA","permalink":"https://wizardtot.github.io/categories/PA/"}],"tags":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://wizardtot.github.io/tags/TinyKV/"},{"name":"MIT6.824","slug":"MIT6-824","permalink":"https://wizardtot.github.io/tags/MIT6-824/"},{"name":"实验","slug":"实验","permalink":"https://wizardtot.github.io/tags/%E5%AE%9E%E9%AA%8C/"},{"name":"PA","slug":"PA","permalink":"https://wizardtot.github.io/tags/PA/"}]}